{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b93cd3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9656421",
   "metadata": {},
   "source": [
    "# What Is Groq "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91f50bc",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    "# 🟢 GROQ LPU | Language Processing Unit\n",
    "\n",
    "## ✅ إيه هو Groq؟\n",
    "- **Groq Inc.** شركة متخصصة في تصميم معالجات (Accelerators) لتشغيل **AI Models** بسرعات رهيبة.\n",
    "- مش GPU زي NVIDIA ولا TPU زي Google.\n",
    "- عاملين معمارية جديدة اسمها **LPU (Language Processing Unit)**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 LPU = Language Processing Unit\n",
    "- معمارية مصممة مخصوص لتشغيل **LLMs** بسرعة عالية و **latency منخفض جدًا**.\n",
    "- بيعتمد على **Single Large Matrix Multiply Engine** بدل من Cores كتير زي GPUs.\n",
    "- بيحقق **deterministic execution** ➔ مفيش delays عشوائية.\n",
    "- مخصص أكتر لـ **Inference** مش Training.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 فكرة LPU ببساطة\n",
    "- مش parallel cores زي الـ GPUs.\n",
    "- بيعتمد على **dataflow architecture** (كل العمليات ماشية في pipeline سريع).\n",
    "- بيوصل لـ **ultra-low latency inference** (ميلي ثانية قليلة جدًا).\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Groq LPU vs NVIDIA GPU (مقارنة)\n",
    "| | **GPU (NVIDIA A100)** | **Groq LPU** |\n",
    "|---|---------------------|--------------|\n",
    "| **Throughput** | عالي | عالي جدًا |\n",
    "| **Latency** | ممكن يتأخر | ultra-low latency |\n",
    "| **Architecture** | Parallel Cores | Single Dataflow Engine |\n",
    "| **Use-case** | Training & Inference | Inference-focused (LLM) |\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 استخدامات Groq LPU\n",
    "- AI Chatbots (ChatGPT Alternatives).\n",
    "- Real-time AI Agents.\n",
    "- Robotics & Autonomous Systems.\n",
    "- أي حاجة محتاجة **استجابات لحظية وسريعة**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🟡 ليه الناس مهتمة بـ Groq؟\n",
    "- للشركات اللي محتاجة **Real-time Responses** (زي البورصة، السيارات الذكية).\n",
    "- GPUs ساعات latency بتاعها مش كافي.\n",
    "- **Groq LPU** بيحقق سرعة خرافية من غير ما يضحي بالدقة.\n",
    "\n",
    "---\n",
    "\n",
    "## 📝 ملحوظة\n",
    "- Groq عملت **chat service** بيشتغل على LPU.\n",
    "- أسرع فعليًا من GPT-3.5 في وقت الاستجابة.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25188aaf",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    "# 🏗️ إيه هي الـ Infrastructure؟\n",
    "\n",
    "## ✅ التعريف:\n",
    "> **Infrastructure = البنية التحتية**  \n",
    "هي كل حاجة أساسية بنبني عليها أنظمة الـ IT و الـ AI  \n",
    "(زي ما العمارة لازم يبقى ليها أساس).\n",
    "\n",
    "---\n",
    "\n",
    "## 🖥️ مكونات الـ Infrastructure (في التكنولوجيا):\n",
    "### 1. **Hardware (العتاد):**\n",
    "- سيرفرات (Servers)\n",
    "- GPUs / TPUs / LPUs (زي Groq LPU)\n",
    "- Networking Devices (Routers, Switches)\n",
    "- Storage (Hard Drives, SSDs)\n",
    "\n",
    "### 2. **Software Platforms:**\n",
    "- أنظمة تشغيل (Linux, Windows)\n",
    "- Cloud Platforms (AWS, Azure, GCP)\n",
    "- Kubernetes, Docker (لإدارة الـ Containers)\n",
    "\n",
    "### 3. **Services & Tools:**\n",
    "- Databases (MySQL, MongoDB)\n",
    "- APIs و Gateways\n",
    "- Monitoring Tools (Grafana, Prometheus)\n",
    "\n",
    "### 4. **DevOps & CI/CD:**\n",
    "- Automation (Jenkins, GitLab CI)\n",
    "- Version Control (Git)\n",
    "- Deployment Pipelines\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 ليه Infrastructure مهمة؟\n",
    "- بتوفر **البيئة الأساسية** اللي الأنظمة بتشتغل عليها.\n",
    "- بتحقق:\n",
    "  - 🔹 سرعة (Performance)\n",
    "  - 🔹 استقرار (Reliability)\n",
    "  - 🔹 أمان (Security)\n",
    "  - 🔹 قابلية التوسع (Scalability)\n",
    "\n",
    "---\n",
    "\n",
    "# 🚀 علاقة Groq LPU بالـ Infrastructure\n",
    "\n",
    "## 🟢 Groq LPU = Hardware Accelerator\n",
    "- **Groq LPU** عبارة عن معالج (Chip) متخصص في تشغيل **Large Language Models** بسرعة.\n",
    "- جزء من **Hardware Layer** في الـ Infrastructure.\n",
    "- هدفه الأساسي: **Ultra-low Latency Inference**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🖥️ مكان Groq LPU في الـ Infrastructure Stack\n",
    "\n",
    "| Layer | Example | علاقة Groq LPU |\n",
    "|--------|---------|---------------|\n",
    "| **Applications** | AI Chatbot, Copilot | بيشتغلوا فوق الـ LPU |\n",
    "| **AI Models** | LLaMa, Mistral | بتعمل Inference على LPU |\n",
    "| **AI Accelerators** | Groq LPU, NVIDIA GPU | 🟡 Groq LPU موجود هنا |\n",
    "| **Servers / Data Centers** | Cloud Servers | LPU بيركب في السيرفرات |\n",
    "| **Networking & Storage** | Routers, Databases | بيتوصل معاهم |\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 وظيفة Groq LPU جوه الـ Infrastructure\n",
    "- بيقوم بتسريع **تشغيل النماذج (Inference)**.\n",
    "- بيتكامل مع باقي مكونات الـ Infrastructure (شبكات، سيرفرات، Storage).\n",
    "- بيحقق Latency منخفض جدًا.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔥 الفرق بين GPU Infrastructure و Groq LPU Infrastructure\n",
    "\n",
    "| | **GPU (NVIDIA)** | **Groq LPU** |\n",
    "|---|-----------------|-------------|\n",
    "| **Architecture** | Parallel Cores | Dataflow Matrix Engine |\n",
    "| **Latency** | ممكن يتأخر | Ultra-Low Latency |\n",
    "| **Use-case** | Training + Inference | Inference فقط |\n",
    "| **Integration** | Cloud, AI Farms | نفس الكلام but optimized for LPU |\n",
    "\n",
    "---\n",
    "\n",
    "## 📝 الخلاصة:\n",
    "> **Infrastructure = كل اللي بيشغل النظام (عتاد + سوفتوير + شبكات).**  \n",
    "> **Groq LPU** هو قطعة Hardware جوه الـ Infrastructure مخصصة لتسريع الـ AI Models.  \n",
    "> من غير Infrastructure كاملة، الـ LPU مش هيعرف يطلع كل إمكانياته.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0c2655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key = os.getenv('GROQ_API_KEY') # to get groq api key\n",
    "\n",
    "# if use openai we do same thins\n",
    "# openai_api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0937063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000121DDCA3170>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000121DDE28F80>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "# if use openai we do same thins\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# to call specific model from groq\n",
    "model = ChatGroq(model=\"Gemma2-9b-It\", groq_api_key=groq_api_key) # get name model from groq website\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "018f1f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As an AI, I don't have feelings or experiences like humans do. But I'm here and ready to assist you! How can I help you today?\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 12, 'total_tokens': 49, 'completion_time': 0.067272727, 'prompt_time': 0.001929916, 'queue_time': 0.16359846, 'total_time': 0.069202643}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--9adc361b-8415-4cce-91c7-d515eeb5c04f-0', usage_metadata={'input_tokens': 12, 'output_tokens': 37, 'total_tokens': 49})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke('How Are You')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5ef135",
   "metadata": {},
   "source": [
    "- لاحظ ان اللى بيرجع من الموديل هو من نوع Ai Message \n",
    "- علشان ناخد ال content بس كنا نبستخدم ال Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19aacee",
   "metadata": {},
   "source": [
    "# Humman Messages & SystemMessages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f191886",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    "- SystemMessage => \n",
    "- دي رسائل للموديل بنقوله هيتصرف ازاى او هنخصصه ازاى او نحدد دوره زى نقوله انت خبير ذكاء اصطناعى وجاوب على الاساله دي\n",
    "- HumanMessage => دا ال i/p الحقيقي بتاع ال user \n",
    "\n",
    "- prompt template => اللى كنا بنعمله قبل كدا كان ف الحقيقه بيعمل النوعين اللى فاتو دول كنا بنديله فعلا رساله للنظام مع الدخل بتاع المستخدم\n",
    "- هنعملها يدوى بقى دلوقتي وهنلاقي ان الطريقه بتاعة ال Prompt احسن\n",
    "\n",
    "- بالمناسبه نوع الرد اللى بيجي من الموديل بيبقي AiMessages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a2c8cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The most common way to say \"Hello How Are You?\" in French is:\\n\\n**Bonjour, comment allez-vous ?** \\n\\nHere\\'s a breakdown:\\n\\n* **Bonjour:** Hello\\n* **comment:** how\\n* **allez-vous:**  are you (formal, polite)\\n\\n\\nLet me know if you\\'d like some other ways to say it! 😊\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 81, 'prompt_tokens': 21, 'total_tokens': 102, 'completion_time': 0.147272727, 'prompt_time': 0.002506044, 'queue_time': 0.16203005599999998, 'total_time': 0.149778771}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--2a109651-f690-4e63-817f-56b46f9c6217-0', usage_metadata={'input_tokens': 21, 'output_tokens': 81, 'total_tokens': 102})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "messages = [\n",
    "    SystemMessage(content='Translate the following from english to french'),\n",
    "    HumanMessage(content='Hello How Are You?')\n",
    "]\n",
    "\n",
    "# SystemMessage => instruction to model\n",
    "# HumanMessage => input by user\n",
    "\n",
    "respone = model.invoke(messages)\n",
    "respone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb420d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The most common way to say \"Hello How Are You?\" in French is:\\n\\n**Bonjour, comment allez-vous ?** \\n\\nHere\\'s a breakdown:\\n\\n* **Bonjour:** Hello\\n* **comment:** how\\n* **allez-vous:**  are you (formal, polite)\\n\\n\\nLet me know if you\\'d like some other ways to say it! 😊\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respone.content  # To Get Content Of Message Only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56cc421b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The most common way to say \"Hello How Are You?\" in French is:\\n\\n**Bonjour, comment allez-vous ?** \\n\\nHere\\'s a breakdown:\\n\\n* **Bonjour:** Hello\\n* **comment:** how\\n* **allez-vous:**  are you (formal, polite)\\n\\n\\nLet me know if you\\'d like some other ways to say it! 😊\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another Way To Get Content Only We Can Use StrOutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser = StrOutputParser() # We Can Create Custom Parser\n",
    "parser.invoke(respone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93cb3b6",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    "# LangChain Language Expression to Chain Components Together\n",
    "\n",
    "---\n",
    "\n",
    "## ما هو LangChain؟\n",
    "\n",
    "- **LangChain** هو إطار عمل (framework) لبناء تطبيقات تعتمد على نماذج اللغة الكبيرة (LLMs).\n",
    "- يهدف إلى تسهيل **ربط (chain)** مكونات مختلفة لإنشاء سير عمل ذكي ومتسلسل.\n",
    "\n",
    "---\n",
    "\n",
    "## معنى \"to chain components together\"\n",
    "\n",
    "- في LangChain يوجد مكونات متعددة مثل:\n",
    "  - **Prompts** (قوالب النصوص)\n",
    "  - **LLMs** (نماذج اللغة)\n",
    "  - **Memory** (الذاكرة)\n",
    "  - **Agents** (الوكلاء)\n",
    "  - **Tools** (أدوات خارجية)\n",
    "\n",
    "- كلمة **\"chain\"** تعني ربط هذه المكونات معًا بحيث تعمل بشكل تسلسلي أو متتابع.\n",
    "- تشبه فكرة ربط عدة مراحل معًا في خط إنتاج (pipeline).\n",
    "\n",
    "---\n",
    "\n",
    "## لماذا نستخدم Chain؟\n",
    "\n",
    "- لبناء تطبيقات معقدة من مكونات بسيطة.\n",
    "- لتنظيم الكود وجعله أكثر مرونة وقابلية لإعادة الاستخدام.\n",
    "- للتحكم في تدفق البيانات\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84eed8a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here are some ways to translate \"Hello, How are you?\" to French, depending on the level of formality:\\n\\n**Formal:**\\n\\n* **Bonjour, comment allez-vous ?** (This is the most formal and polite option.)\\n\\n**Informal:**\\n\\n* **Salut, ça va ?** (This is a very common and casual greeting.)\\n* **Coucou, comment vas-tu ?** (This is a friendly and informal greeting, typically used with people you know well.)\\n\\n**Other options:**\\n\\n* **Bonjour, comment vous portez-vous ?** (A bit more formal than \"comment allez-vous?\")\\n* **Ça roule ?** (Slang, meaning \"How\\'s it going?\")\\n\\n\\n\\nLet me know if you have any other phrases you\\'d like translated!\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using LCEL we can chain th components\n",
    "chain = model|parser  # first messages go to the model then the output go to the parser\n",
    "chain.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# بدل ما نبعتله مجموعه من ال messages \n",
    "# زي اللى فاتت كدا ف طريقه احسن وهى اننا بنكتب Prompt Template \n",
    "# Prompt take combine input by user and some application logic\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "gereric_template = \"Translate The Following Into {language}\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', gereric_template),\n",
    "        ('user', '{text}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# لاحظ كدا محتاج لما انادى على الموديل اديله language, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Translate The Following Into French', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = prompt.invoke({'language': 'French', 'text': 'Hello'})\n",
    "result\n",
    "#ChatPromptValue => contain and translate prompt to SystemMessage, HummanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "361e9fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate The Following Into French', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hello', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_messages() # The We Passed By Hand Before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c27c9d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bonjour \\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets create chain\n",
    "chain = prompt|model|parser\n",
    "chain.invoke({\n",
    "    'language':'french',\n",
    "    'text':'Hello'\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ab8566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b611999a",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    "# معنى `add_routes` في LangServe\n",
    "\n",
    "---\n",
    "\n",
    "## ١. إيه هو LangServe؟\n",
    "\n",
    "- **LangServe** هو سيرفر (Server) لتشغيل تطبيقات LangChain كنماذج أو خدمات API.\n",
    "- بيسمح لك تعرض الـ Chains أو الـ Agents بتوعك كـ APIs تقدر تستخدمها في تطبيقات مختلفة.\n",
    "\n",
    "---\n",
    "\n",
    "## ٢. إيه وظيفة `add_routes`؟\n",
    "\n",
    "- `add_routes` هي دالة (function) تستخدمها عشان:\n",
    "  - تضيف **Routes (مسارات)** جديدة للسيرفر.\n",
    "  - كل Route هو نقطة دخول (Endpoint) بتستقبل طلبات (Requests) وبترد عليهم.\n",
    "- بتربط بين **URL معين** و **كود معالجة الطلب** (مثل Chain أو Agent معين).\n",
    "\n",
    "---\n",
    "\n",
    "## ٣. ليه نستخدم `add_routes`؟\n",
    "\n",
    "- عشان تبني سيرفر API مرن.\n",
    "- تقدر تضيف مسارات مختلفة لخدمات متعددة في نفس السيرفر.\n",
    "- تسهل إدارة الطلبات وتوجيهها للكود المناسب.\n",
    "\n",
    "---\n",
    "\n",
    "## ٤. مثال مبسط\n",
    "\n",
    "```python\n",
    "from langserve import LangServe\n",
    "\n",
    "app = LangServe()\n",
    "\n",
    "# نضيف Route جديد بيسمع على /chat\n",
    "@app.add_routes(\"/chat\")\n",
    "def chat_handler(request):\n",
    "    # هنا بتتعامل مع الطلب\n",
    "    return {\"message\": \"مرحبا من نقطة دخول /chat\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9096c5",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    "# ملخص سريع عن API و Schema\n",
    "\n",
    "---\n",
    "\n",
    "## 1. API (واجهة برمجة التطبيقات)\n",
    "\n",
    "- **API** هي طريقة تواصل بين برنامجين أو نظامين مختلفين.\n",
    "- بتسمح لتطبيقات مختلفة تبادل البيانات وتنفيذ وظائف عبر الإنترنت أو داخليًا.\n",
    "- ممكن تكون RESTful API، GraphQL، أو غيرها.\n",
    "- مثال: لما تستخدم تطبيق يطلب بيانات الطقس من سيرفر خارجي، ده بيتم عن طريق API.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Schema (مخطط البيانات)\n",
    "\n",
    "- **Schema** هو تعريف أو خريطة توضح شكل وتركيب البيانات.\n",
    "- بيحدد الحقول، نوع البيانات (نص، رقم، تاريخ...)، والقواعد اللي لازم تتبعها البيانات.\n",
    "- بيستخدم في قواعد البيانات، APIs، ونماذج البيانات المختلفة.\n",
    "- مثال: مخطط JSON Schema يوضح إن حقل \"name\" لازم يكون نص وحقل \"age\" رقم.\n",
    "\n",
    "---\n",
    "\n",
    "## الفرق ببساطة:\n",
    "\n",
    "- **API** هو الوسيط اللي بين التطبيقات.\n",
    "- **Schema** هو شكل البيانات اللي بتنتقل بين التطبيقات أو بتخزن.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce47349",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
