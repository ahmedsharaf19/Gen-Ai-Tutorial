{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b93cd3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9656421",
   "metadata": {},
   "source": [
    "# What Is Groq "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91f50bc",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    "# ğŸŸ¢ GROQ LPU | Language Processing Unit\n",
    "\n",
    "## âœ… Ø¥ÙŠÙ‡ Ù‡Ùˆ GroqØŸ\n",
    "- **Groq Inc.** Ø´Ø±ÙƒØ© Ù…ØªØ®ØµØµØ© ÙÙŠ ØªØµÙ…ÙŠÙ… Ù…Ø¹Ø§Ù„Ø¬Ø§Øª (Accelerators) Ù„ØªØ´ØºÙŠÙ„ **AI Models** Ø¨Ø³Ø±Ø¹Ø§Øª Ø±Ù‡ÙŠØ¨Ø©.\n",
    "- Ù…Ø´ GPU Ø²ÙŠ NVIDIA ÙˆÙ„Ø§ TPU Ø²ÙŠ Google.\n",
    "- Ø¹Ø§Ù…Ù„ÙŠÙ† Ù…Ø¹Ù…Ø§Ø±ÙŠØ© Ø¬Ø¯ÙŠØ¯Ø© Ø§Ø³Ù…Ù‡Ø§ **LPU (Language Processing Unit)**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ LPU = Language Processing Unit\n",
    "- Ù…Ø¹Ù…Ø§Ø±ÙŠØ© Ù…ØµÙ…Ù…Ø© Ù…Ø®ØµÙˆØµ Ù„ØªØ´ØºÙŠÙ„ **LLMs** Ø¨Ø³Ø±Ø¹Ø© Ø¹Ø§Ù„ÙŠØ© Ùˆ **latency Ù…Ù†Ø®ÙØ¶ Ø¬Ø¯Ù‹Ø§**.\n",
    "- Ø¨ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ **Single Large Matrix Multiply Engine** Ø¨Ø¯Ù„ Ù…Ù† Cores ÙƒØªÙŠØ± Ø²ÙŠ GPUs.\n",
    "- Ø¨ÙŠØ­Ù‚Ù‚ **deterministic execution** â” Ù…ÙÙŠØ´ delays Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©.\n",
    "- Ù…Ø®ØµØµ Ø£ÙƒØªØ± Ù„Ù€ **Inference** Ù…Ø´ Training.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  ÙÙƒØ±Ø© LPU Ø¨Ø¨Ø³Ø§Ø·Ø©\n",
    "- Ù…Ø´ parallel cores Ø²ÙŠ Ø§Ù„Ù€ GPUs.\n",
    "- Ø¨ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ **dataflow architecture** (ÙƒÙ„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ù…Ø§Ø´ÙŠØ© ÙÙŠ pipeline Ø³Ø±ÙŠØ¹).\n",
    "- Ø¨ÙŠÙˆØµÙ„ Ù„Ù€ **ultra-low latency inference** (Ù…ÙŠÙ„ÙŠ Ø«Ø§Ù†ÙŠØ© Ù‚Ù„ÙŠÙ„Ø© Ø¬Ø¯Ù‹Ø§).\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š Groq LPU vs NVIDIA GPU (Ù…Ù‚Ø§Ø±Ù†Ø©)\n",
    "| | **GPU (NVIDIA A100)** | **Groq LPU** |\n",
    "|---|---------------------|--------------|\n",
    "| **Throughput** | Ø¹Ø§Ù„ÙŠ | Ø¹Ø§Ù„ÙŠ Ø¬Ø¯Ù‹Ø§ |\n",
    "| **Latency** | Ù…Ù…ÙƒÙ† ÙŠØªØ£Ø®Ø± | ultra-low latency |\n",
    "| **Architecture** | Parallel Cores | Single Dataflow Engine |\n",
    "| **Use-case** | Training & Inference | Inference-focused (LLM) |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Øª Groq LPU\n",
    "- AI Chatbots (ChatGPT Alternatives).\n",
    "- Real-time AI Agents.\n",
    "- Robotics & Autonomous Systems.\n",
    "- Ø£ÙŠ Ø­Ø§Ø¬Ø© Ù…Ø­ØªØ§Ø¬Ø© **Ø§Ø³ØªØ¬Ø§Ø¨Ø§Øª Ù„Ø­Ø¸ÙŠØ© ÙˆØ³Ø±ÙŠØ¹Ø©**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŸ¡ Ù„ÙŠÙ‡ Ø§Ù„Ù†Ø§Ø³ Ù…Ù‡ØªÙ…Ø© Ø¨Ù€ GroqØŸ\n",
    "- Ù„Ù„Ø´Ø±ÙƒØ§Øª Ø§Ù„Ù„ÙŠ Ù…Ø­ØªØ§Ø¬Ø© **Real-time Responses** (Ø²ÙŠ Ø§Ù„Ø¨ÙˆØ±ØµØ©ØŒ Ø§Ù„Ø³ÙŠØ§Ø±Ø§Øª Ø§Ù„Ø°ÙƒÙŠØ©).\n",
    "- GPUs Ø³Ø§Ø¹Ø§Øª latency Ø¨ØªØ§Ø¹Ù‡Ø§ Ù…Ø´ ÙƒØ§ÙÙŠ.\n",
    "- **Groq LPU** Ø¨ÙŠØ­Ù‚Ù‚ Ø³Ø±Ø¹Ø© Ø®Ø±Ø§ÙÙŠØ© Ù…Ù† ØºÙŠØ± Ù…Ø§ ÙŠØ¶Ø­ÙŠ Ø¨Ø§Ù„Ø¯Ù‚Ø©.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Ù…Ù„Ø­ÙˆØ¸Ø©\n",
    "- Groq Ø¹Ù…Ù„Øª **chat service** Ø¨ÙŠØ´ØªØºÙ„ Ø¹Ù„Ù‰ LPU.\n",
    "- Ø£Ø³Ø±Ø¹ ÙØ¹Ù„ÙŠÙ‹Ø§ Ù…Ù† GPT-3.5 ÙÙŠ ÙˆÙ‚Øª Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25188aaf",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    "# ğŸ—ï¸ Ø¥ÙŠÙ‡ Ù‡ÙŠ Ø§Ù„Ù€ InfrastructureØŸ\n",
    "\n",
    "## âœ… Ø§Ù„ØªØ¹Ø±ÙŠÙ:\n",
    "> **Infrastructure = Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªØ­ØªÙŠØ©**  \n",
    "Ù‡ÙŠ ÙƒÙ„ Ø­Ø§Ø¬Ø© Ø£Ø³Ø§Ø³ÙŠØ© Ø¨Ù†Ø¨Ù†ÙŠ Ø¹Ù„ÙŠÙ‡Ø§ Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ù€ IT Ùˆ Ø§Ù„Ù€ AI  \n",
    "(Ø²ÙŠ Ù…Ø§ Ø§Ù„Ø¹Ù…Ø§Ø±Ø© Ù„Ø§Ø²Ù… ÙŠØ¨Ù‚Ù‰ Ù„ÙŠÙ‡Ø§ Ø£Ø³Ø§Ø³).\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ–¥ï¸ Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ù€ Infrastructure (ÙÙŠ Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§):\n",
    "### 1. **Hardware (Ø§Ù„Ø¹ØªØ§Ø¯):**\n",
    "- Ø³ÙŠØ±ÙØ±Ø§Øª (Servers)\n",
    "- GPUs / TPUs / LPUs (Ø²ÙŠ Groq LPU)\n",
    "- Networking Devices (Routers, Switches)\n",
    "- Storage (Hard Drives, SSDs)\n",
    "\n",
    "### 2. **Software Platforms:**\n",
    "- Ø£Ù†Ø¸Ù…Ø© ØªØ´ØºÙŠÙ„ (Linux, Windows)\n",
    "- Cloud Platforms (AWS, Azure, GCP)\n",
    "- Kubernetes, Docker (Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù€ Containers)\n",
    "\n",
    "### 3. **Services & Tools:**\n",
    "- Databases (MySQL, MongoDB)\n",
    "- APIs Ùˆ Gateways\n",
    "- Monitoring Tools (Grafana, Prometheus)\n",
    "\n",
    "### 4. **DevOps & CI/CD:**\n",
    "- Automation (Jenkins, GitLab CI)\n",
    "- Version Control (Git)\n",
    "- Deployment Pipelines\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Ù„ÙŠÙ‡ Infrastructure Ù…Ù‡Ù…Ø©ØŸ\n",
    "- Ø¨ØªÙˆÙØ± **Ø§Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©** Ø§Ù„Ù„ÙŠ Ø§Ù„Ø£Ù†Ø¸Ù…Ø© Ø¨ØªØ´ØªØºÙ„ Ø¹Ù„ÙŠÙ‡Ø§.\n",
    "- Ø¨ØªØ­Ù‚Ù‚:\n",
    "  - ğŸ”¹ Ø³Ø±Ø¹Ø© (Performance)\n",
    "  - ğŸ”¹ Ø§Ø³ØªÙ‚Ø±Ø§Ø± (Reliability)\n",
    "  - ğŸ”¹ Ø£Ù…Ø§Ù† (Security)\n",
    "  - ğŸ”¹ Ù‚Ø§Ø¨Ù„ÙŠØ© Ø§Ù„ØªÙˆØ³Ø¹ (Scalability)\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸš€ Ø¹Ù„Ø§Ù‚Ø© Groq LPU Ø¨Ø§Ù„Ù€ Infrastructure\n",
    "\n",
    "## ğŸŸ¢ Groq LPU = Hardware Accelerator\n",
    "- **Groq LPU** Ø¹Ø¨Ø§Ø±Ø© Ø¹Ù† Ù…Ø¹Ø§Ù„Ø¬ (Chip) Ù…ØªØ®ØµØµ ÙÙŠ ØªØ´ØºÙŠÙ„ **Large Language Models** Ø¨Ø³Ø±Ø¹Ø©.\n",
    "- Ø¬Ø²Ø¡ Ù…Ù† **Hardware Layer** ÙÙŠ Ø§Ù„Ù€ Infrastructure.\n",
    "- Ù‡Ø¯ÙÙ‡ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ: **Ultra-low Latency Inference**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ–¥ï¸ Ù…ÙƒØ§Ù† Groq LPU ÙÙŠ Ø§Ù„Ù€ Infrastructure Stack\n",
    "\n",
    "| Layer | Example | Ø¹Ù„Ø§Ù‚Ø© Groq LPU |\n",
    "|--------|---------|---------------|\n",
    "| **Applications** | AI Chatbot, Copilot | Ø¨ÙŠØ´ØªØºÙ„ÙˆØ§ ÙÙˆÙ‚ Ø§Ù„Ù€ LPU |\n",
    "| **AI Models** | LLaMa, Mistral | Ø¨ØªØ¹Ù…Ù„ Inference Ø¹Ù„Ù‰ LPU |\n",
    "| **AI Accelerators** | Groq LPU, NVIDIA GPU | ğŸŸ¡ Groq LPU Ù…ÙˆØ¬ÙˆØ¯ Ù‡Ù†Ø§ |\n",
    "| **Servers / Data Centers** | Cloud Servers | LPU Ø¨ÙŠØ±ÙƒØ¨ ÙÙŠ Ø§Ù„Ø³ÙŠØ±ÙØ±Ø§Øª |\n",
    "| **Networking & Storage** | Routers, Databases | Ø¨ÙŠØªÙˆØµÙ„ Ù…Ø¹Ø§Ù‡Ù… |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ ÙˆØ¸ÙŠÙØ© Groq LPU Ø¬ÙˆÙ‡ Ø§Ù„Ù€ Infrastructure\n",
    "- Ø¨ÙŠÙ‚ÙˆÙ… Ø¨ØªØ³Ø±ÙŠØ¹ **ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ (Inference)**.\n",
    "- Ø¨ÙŠØªÙƒØ§Ù…Ù„ Ù…Ø¹ Ø¨Ø§Ù‚ÙŠ Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ù€ Infrastructure (Ø´Ø¨ÙƒØ§ØªØŒ Ø³ÙŠØ±ÙØ±Ø§ØªØŒ Storage).\n",
    "- Ø¨ÙŠØ­Ù‚Ù‚ Latency Ù…Ù†Ø®ÙØ¶ Ø¬Ø¯Ù‹Ø§.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¥ Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† GPU Infrastructure Ùˆ Groq LPU Infrastructure\n",
    "\n",
    "| | **GPU (NVIDIA)** | **Groq LPU** |\n",
    "|---|-----------------|-------------|\n",
    "| **Architecture** | Parallel Cores | Dataflow Matrix Engine |\n",
    "| **Latency** | Ù…Ù…ÙƒÙ† ÙŠØªØ£Ø®Ø± | Ultra-Low Latency |\n",
    "| **Use-case** | Training + Inference | Inference ÙÙ‚Ø· |\n",
    "| **Integration** | Cloud, AI Farms | Ù†ÙØ³ Ø§Ù„ÙƒÙ„Ø§Ù… but optimized for LPU |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Ø§Ù„Ø®Ù„Ø§ØµØ©:\n",
    "> **Infrastructure = ÙƒÙ„ Ø§Ù„Ù„ÙŠ Ø¨ÙŠØ´ØºÙ„ Ø§Ù„Ù†Ø¸Ø§Ù… (Ø¹ØªØ§Ø¯ + Ø³ÙˆÙØªÙˆÙŠØ± + Ø´Ø¨ÙƒØ§Øª).**  \n",
    "> **Groq LPU** Ù‡Ùˆ Ù‚Ø·Ø¹Ø© Hardware Ø¬ÙˆÙ‡ Ø§Ù„Ù€ Infrastructure Ù…Ø®ØµØµØ© Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ù€ AI Models.  \n",
    "> Ù…Ù† ØºÙŠØ± Infrastructure ÙƒØ§Ù…Ù„Ø©ØŒ Ø§Ù„Ù€ LPU Ù…Ø´ Ù‡ÙŠØ¹Ø±Ù ÙŠØ·Ù„Ø¹ ÙƒÙ„ Ø¥Ù…ÙƒØ§Ù†ÙŠØ§ØªÙ‡.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0c2655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key = os.getenv('GROQ_API_KEY') # to get groq api key\n",
    "\n",
    "# if use openai we do same thins\n",
    "# openai_api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0937063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000121DDCA3170>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000121DDE28F80>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "# if use openai we do same thins\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# to call specific model from groq\n",
    "model = ChatGroq(model=\"Gemma2-9b-It\", groq_api_key=groq_api_key) # get name model from groq website\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "018f1f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As an AI, I don't have feelings or experiences like humans do. But I'm here and ready to assist you! How can I help you today?\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 12, 'total_tokens': 49, 'completion_time': 0.067272727, 'prompt_time': 0.001929916, 'queue_time': 0.16359846, 'total_time': 0.069202643}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--9adc361b-8415-4cce-91c7-d515eeb5c04f-0', usage_metadata={'input_tokens': 12, 'output_tokens': 37, 'total_tokens': 49})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke('How Are You')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5ef135",
   "metadata": {},
   "source": [
    "- Ù„Ø§Ø­Ø¸ Ø§Ù† Ø§Ù„Ù„Ù‰ Ø¨ÙŠØ±Ø¬Ø¹ Ù…Ù† Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ù‡Ùˆ Ù…Ù† Ù†ÙˆØ¹ Ai Message \n",
    "- Ø¹Ù„Ø´Ø§Ù† Ù†Ø§Ø®Ø¯ Ø§Ù„ content Ø¨Ø³ ÙƒÙ†Ø§ Ù†Ø¨Ø³ØªØ®Ø¯Ù… Ø§Ù„ Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19aacee",
   "metadata": {},
   "source": [
    "# Humman Messages & SystemMessages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f191886",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    "- SystemMessage => \n",
    "- Ø¯ÙŠ Ø±Ø³Ø§Ø¦Ù„ Ù„Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø¨Ù†Ù‚ÙˆÙ„Ù‡ Ù‡ÙŠØªØµØ±Ù Ø§Ø²Ø§Ù‰ Ø§Ùˆ Ù‡Ù†Ø®ØµØµÙ‡ Ø§Ø²Ø§Ù‰ Ø§Ùˆ Ù†Ø­Ø¯Ø¯ Ø¯ÙˆØ±Ù‡ Ø²Ù‰ Ù†Ù‚ÙˆÙ„Ù‡ Ø§Ù†Øª Ø®Ø¨ÙŠØ± Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹Ù‰ ÙˆØ¬Ø§ÙˆØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³Ø§Ù„Ù‡ Ø¯ÙŠ\n",
    "- HumanMessage => Ø¯Ø§ Ø§Ù„ i/p Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ Ø¨ØªØ§Ø¹ Ø§Ù„ user \n",
    "\n",
    "- prompt template => Ø§Ù„Ù„Ù‰ ÙƒÙ†Ø§ Ø¨Ù†Ø¹Ù…Ù„Ù‡ Ù‚Ø¨Ù„ ÙƒØ¯Ø§ ÙƒØ§Ù† Ù Ø§Ù„Ø­Ù‚ÙŠÙ‚Ù‡ Ø¨ÙŠØ¹Ù…Ù„ Ø§Ù„Ù†ÙˆØ¹ÙŠÙ† Ø§Ù„Ù„Ù‰ ÙØ§ØªÙˆ Ø¯ÙˆÙ„ ÙƒÙ†Ø§ Ø¨Ù†Ø¯ÙŠÙ„Ù‡ ÙØ¹Ù„Ø§ Ø±Ø³Ø§Ù„Ù‡ Ù„Ù„Ù†Ø¸Ø§Ù… Ù…Ø¹ Ø§Ù„Ø¯Ø®Ù„ Ø¨ØªØ§Ø¹ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…\n",
    "- Ù‡Ù†Ø¹Ù…Ù„Ù‡Ø§ ÙŠØ¯ÙˆÙ‰ Ø¨Ù‚Ù‰ Ø¯Ù„ÙˆÙ‚ØªÙŠ ÙˆÙ‡Ù†Ù„Ø§Ù‚ÙŠ Ø§Ù† Ø§Ù„Ø·Ø±ÙŠÙ‚Ù‡ Ø¨ØªØ§Ø¹Ø© Ø§Ù„ Prompt Ø§Ø­Ø³Ù†\n",
    "\n",
    "- Ø¨Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ù‡ Ù†ÙˆØ¹ Ø§Ù„Ø±Ø¯ Ø§Ù„Ù„Ù‰ Ø¨ÙŠØ¬ÙŠ Ù…Ù† Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø¨ÙŠØ¨Ù‚ÙŠ AiMessages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a2c8cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The most common way to say \"Hello How Are You?\" in French is:\\n\\n**Bonjour, comment allez-vous ?** \\n\\nHere\\'s a breakdown:\\n\\n* **Bonjour:** Hello\\n* **comment:** how\\n* **allez-vous:**  are you (formal, polite)\\n\\n\\nLet me know if you\\'d like some other ways to say it! ğŸ˜Š\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 81, 'prompt_tokens': 21, 'total_tokens': 102, 'completion_time': 0.147272727, 'prompt_time': 0.002506044, 'queue_time': 0.16203005599999998, 'total_time': 0.149778771}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--2a109651-f690-4e63-817f-56b46f9c6217-0', usage_metadata={'input_tokens': 21, 'output_tokens': 81, 'total_tokens': 102})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "messages = [\n",
    "    SystemMessage(content='Translate the following from english to french'),\n",
    "    HumanMessage(content='Hello How Are You?')\n",
    "]\n",
    "\n",
    "# SystemMessage => instruction to model\n",
    "# HumanMessage => input by user\n",
    "\n",
    "respone = model.invoke(messages)\n",
    "respone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb420d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The most common way to say \"Hello How Are You?\" in French is:\\n\\n**Bonjour, comment allez-vous ?** \\n\\nHere\\'s a breakdown:\\n\\n* **Bonjour:** Hello\\n* **comment:** how\\n* **allez-vous:**  are you (formal, polite)\\n\\n\\nLet me know if you\\'d like some other ways to say it! ğŸ˜Š\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respone.content  # To Get Content Of Message Only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56cc421b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The most common way to say \"Hello How Are You?\" in French is:\\n\\n**Bonjour, comment allez-vous ?** \\n\\nHere\\'s a breakdown:\\n\\n* **Bonjour:** Hello\\n* **comment:** how\\n* **allez-vous:**  are you (formal, polite)\\n\\n\\nLet me know if you\\'d like some other ways to say it! ğŸ˜Š\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another Way To Get Content Only We Can Use StrOutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser = StrOutputParser() # We Can Create Custom Parser\n",
    "parser.invoke(respone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93cb3b6",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    "# LangChain Language Expression to Chain Components Together\n",
    "\n",
    "---\n",
    "\n",
    "## Ù…Ø§ Ù‡Ùˆ LangChainØŸ\n",
    "\n",
    "- **LangChain** Ù‡Ùˆ Ø¥Ø·Ø§Ø± Ø¹Ù…Ù„ (framework) Ù„Ø¨Ù†Ø§Ø¡ ØªØ·Ø¨ÙŠÙ‚Ø§Øª ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„ÙƒØ¨ÙŠØ±Ø© (LLMs).\n",
    "- ÙŠÙ‡Ø¯Ù Ø¥Ù„Ù‰ ØªØ³Ù‡ÙŠÙ„ **Ø±Ø¨Ø· (chain)** Ù…ÙƒÙˆÙ†Ø§Øª Ù…Ø®ØªÙ„ÙØ© Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø³ÙŠØ± Ø¹Ù…Ù„ Ø°ÙƒÙŠ ÙˆÙ…ØªØ³Ù„Ø³Ù„.\n",
    "\n",
    "---\n",
    "\n",
    "## Ù…Ø¹Ù†Ù‰ \"to chain components together\"\n",
    "\n",
    "- ÙÙŠ LangChain ÙŠÙˆØ¬Ø¯ Ù…ÙƒÙˆÙ†Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© Ù…Ø«Ù„:\n",
    "  - **Prompts** (Ù‚ÙˆØ§Ù„Ø¨ Ø§Ù„Ù†ØµÙˆØµ)\n",
    "  - **LLMs** (Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ©)\n",
    "  - **Memory** (Ø§Ù„Ø°Ø§ÙƒØ±Ø©)\n",
    "  - **Agents** (Ø§Ù„ÙˆÙƒÙ„Ø§Ø¡)\n",
    "  - **Tools** (Ø£Ø¯ÙˆØ§Øª Ø®Ø§Ø±Ø¬ÙŠØ©)\n",
    "\n",
    "- ÙƒÙ„Ù…Ø© **\"chain\"** ØªØ¹Ù†ÙŠ Ø±Ø¨Ø· Ù‡Ø°Ù‡ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ù…Ø¹Ù‹Ø§ Ø¨Ø­ÙŠØ« ØªØ¹Ù…Ù„ Ø¨Ø´ÙƒÙ„ ØªØ³Ù„Ø³Ù„ÙŠ Ø£Ùˆ Ù…ØªØªØ§Ø¨Ø¹.\n",
    "- ØªØ´Ø¨Ù‡ ÙÙƒØ±Ø© Ø±Ø¨Ø· Ø¹Ø¯Ø© Ù…Ø±Ø§Ø­Ù„ Ù…Ø¹Ù‹Ø§ ÙÙŠ Ø®Ø· Ø¥Ù†ØªØ§Ø¬ (pipeline).\n",
    "\n",
    "---\n",
    "\n",
    "## Ù„Ù…Ø§Ø°Ø§ Ù†Ø³ØªØ®Ø¯Ù… ChainØŸ\n",
    "\n",
    "- Ù„Ø¨Ù†Ø§Ø¡ ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ù…Ø¹Ù‚Ø¯Ø© Ù…Ù† Ù…ÙƒÙˆÙ†Ø§Øª Ø¨Ø³ÙŠØ·Ø©.\n",
    "- Ù„ØªÙ†Ø¸ÙŠÙ… Ø§Ù„ÙƒÙˆØ¯ ÙˆØ¬Ø¹Ù„Ù‡ Ø£ÙƒØ«Ø± Ù…Ø±ÙˆÙ†Ø© ÙˆÙ‚Ø§Ø¨Ù„ÙŠØ© Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù….\n",
    "- Ù„Ù„ØªØ­ÙƒÙ… ÙÙŠ ØªØ¯ÙÙ‚ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84eed8a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here are some ways to translate \"Hello, How are you?\" to French, depending on the level of formality:\\n\\n**Formal:**\\n\\n* **Bonjour, comment allez-vous ?** (This is the most formal and polite option.)\\n\\n**Informal:**\\n\\n* **Salut, Ã§a va ?** (This is a very common and casual greeting.)\\n* **Coucou, comment vas-tu ?** (This is a friendly and informal greeting, typically used with people you know well.)\\n\\n**Other options:**\\n\\n* **Bonjour, comment vous portez-vous ?** (A bit more formal than \"comment allez-vous?\")\\n* **Ã‡a roule ?** (Slang, meaning \"How\\'s it going?\")\\n\\n\\n\\nLet me know if you have any other phrases you\\'d like translated!\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using LCEL we can chain th components\n",
    "chain = model|parser  # first messages go to the model then the output go to the parser\n",
    "chain.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ø¨Ø¯Ù„ Ù…Ø§ Ù†Ø¨Ø¹ØªÙ„Ù‡ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ù…Ù† Ø§Ù„ messages \n",
    "# Ø²ÙŠ Ø§Ù„Ù„Ù‰ ÙØ§ØªØª ÙƒØ¯Ø§ Ù Ø·Ø±ÙŠÙ‚Ù‡ Ø§Ø­Ø³Ù† ÙˆÙ‡Ù‰ Ø§Ù†Ù†Ø§ Ø¨Ù†ÙƒØªØ¨ Prompt Template \n",
    "# Prompt take combine input by user and some application logic\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "gereric_template = \"Translate The Following Into {language}\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', gereric_template),\n",
    "        ('user', '{text}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Ù„Ø§Ø­Ø¸ ÙƒØ¯Ø§ Ù…Ø­ØªØ§Ø¬ Ù„Ù…Ø§ Ø§Ù†Ø§Ø¯Ù‰ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ø¯ÙŠÙ„Ù‡ language, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Translate The Following Into French', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = prompt.invoke({'language': 'French', 'text': 'Hello'})\n",
    "result\n",
    "#ChatPromptValue => contain and translate prompt to SystemMessage, HummanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "361e9fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate The Following Into French', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hello', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_messages() # The We Passed By Hand Before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c27c9d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bonjour \\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets create chain\n",
    "chain = prompt|model|parser\n",
    "chain.invoke({\n",
    "    'language':'french',\n",
    "    'text':'Hello'\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ab8566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b611999a",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    "# Ù…Ø¹Ù†Ù‰ `add_routes` ÙÙŠ LangServe\n",
    "\n",
    "---\n",
    "\n",
    "## Ù¡. Ø¥ÙŠÙ‡ Ù‡Ùˆ LangServeØŸ\n",
    "\n",
    "- **LangServe** Ù‡Ùˆ Ø³ÙŠØ±ÙØ± (Server) Ù„ØªØ´ØºÙŠÙ„ ØªØ·Ø¨ÙŠÙ‚Ø§Øª LangChain ÙƒÙ†Ù…Ø§Ø°Ø¬ Ø£Ùˆ Ø®Ø¯Ù…Ø§Øª API.\n",
    "- Ø¨ÙŠØ³Ù…Ø­ Ù„Ùƒ ØªØ¹Ø±Ø¶ Ø§Ù„Ù€ Chains Ø£Ùˆ Ø§Ù„Ù€ Agents Ø¨ØªÙˆØ¹Ùƒ ÙƒÙ€ APIs ØªÙ‚Ø¯Ø± ØªØ³ØªØ®Ø¯Ù…Ù‡Ø§ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ù…Ø®ØªÙ„ÙØ©.\n",
    "\n",
    "---\n",
    "\n",
    "## Ù¢. Ø¥ÙŠÙ‡ ÙˆØ¸ÙŠÙØ© `add_routes`ØŸ\n",
    "\n",
    "- `add_routes` Ù‡ÙŠ Ø¯Ø§Ù„Ø© (function) ØªØ³ØªØ®Ø¯Ù…Ù‡Ø§ Ø¹Ø´Ø§Ù†:\n",
    "  - ØªØ¶ÙŠÙ **Routes (Ù…Ø³Ø§Ø±Ø§Øª)** Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„Ø³ÙŠØ±ÙØ±.\n",
    "  - ÙƒÙ„ Route Ù‡Ùˆ Ù†Ù‚Ø·Ø© Ø¯Ø®ÙˆÙ„ (Endpoint) Ø¨ØªØ³ØªÙ‚Ø¨Ù„ Ø·Ù„Ø¨Ø§Øª (Requests) ÙˆØ¨ØªØ±Ø¯ Ø¹Ù„ÙŠÙ‡Ù….\n",
    "- Ø¨ØªØ±Ø¨Ø· Ø¨ÙŠÙ† **URL Ù…Ø¹ÙŠÙ†** Ùˆ **ÙƒÙˆØ¯ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø·Ù„Ø¨** (Ù…Ø«Ù„ Chain Ø£Ùˆ Agent Ù…Ø¹ÙŠÙ†).\n",
    "\n",
    "---\n",
    "\n",
    "## Ù£. Ù„ÙŠÙ‡ Ù†Ø³ØªØ®Ø¯Ù… `add_routes`ØŸ\n",
    "\n",
    "- Ø¹Ø´Ø§Ù† ØªØ¨Ù†ÙŠ Ø³ÙŠØ±ÙØ± API Ù…Ø±Ù†.\n",
    "- ØªÙ‚Ø¯Ø± ØªØ¶ÙŠÙ Ù…Ø³Ø§Ø±Ø§Øª Ù…Ø®ØªÙ„ÙØ© Ù„Ø®Ø¯Ù…Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© ÙÙŠ Ù†ÙØ³ Ø§Ù„Ø³ÙŠØ±ÙØ±.\n",
    "- ØªØ³Ù‡Ù„ Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø·Ù„Ø¨Ø§Øª ÙˆØªÙˆØ¬ÙŠÙ‡Ù‡Ø§ Ù„Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨.\n",
    "\n",
    "---\n",
    "\n",
    "## Ù¤. Ù…Ø«Ø§Ù„ Ù…Ø¨Ø³Ø·\n",
    "\n",
    "```python\n",
    "from langserve import LangServe\n",
    "\n",
    "app = LangServe()\n",
    "\n",
    "# Ù†Ø¶ÙŠÙ Route Ø¬Ø¯ÙŠØ¯ Ø¨ÙŠØ³Ù…Ø¹ Ø¹Ù„Ù‰ /chat\n",
    "@app.add_routes(\"/chat\")\n",
    "def chat_handler(request):\n",
    "    # Ù‡Ù†Ø§ Ø¨ØªØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø·Ù„Ø¨\n",
    "    return {\"message\": \"Ù…Ø±Ø­Ø¨Ø§ Ù…Ù† Ù†Ù‚Ø·Ø© Ø¯Ø®ÙˆÙ„ /chat\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9096c5",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    "# Ù…Ù„Ø®Øµ Ø³Ø±ÙŠØ¹ Ø¹Ù† API Ùˆ Schema\n",
    "\n",
    "---\n",
    "\n",
    "## 1. API (ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª)\n",
    "\n",
    "- **API** Ù‡ÙŠ Ø·Ø±ÙŠÙ‚Ø© ØªÙˆØ§ØµÙ„ Ø¨ÙŠÙ† Ø¨Ø±Ù†Ø§Ù…Ø¬ÙŠÙ† Ø£Ùˆ Ù†Ø¸Ø§Ù…ÙŠÙ† Ù…Ø®ØªÙ„ÙÙŠÙ†.\n",
    "- Ø¨ØªØ³Ù…Ø­ Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ù…Ø®ØªÙ„ÙØ© ØªØ¨Ø§Ø¯Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØªÙ†ÙÙŠØ° ÙˆØ¸Ø§Ø¦Ù Ø¹Ø¨Ø± Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª Ø£Ùˆ Ø¯Ø§Ø®Ù„ÙŠÙ‹Ø§.\n",
    "- Ù…Ù…ÙƒÙ† ØªÙƒÙˆÙ† RESTful APIØŒ GraphQLØŒ Ø£Ùˆ ØºÙŠØ±Ù‡Ø§.\n",
    "- Ù…Ø«Ø§Ù„: Ù„Ù…Ø§ ØªØ³ØªØ®Ø¯Ù… ØªØ·Ø¨ÙŠÙ‚ ÙŠØ·Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø·Ù‚Ø³ Ù…Ù† Ø³ÙŠØ±ÙØ± Ø®Ø§Ø±Ø¬ÙŠØŒ Ø¯Ù‡ Ø¨ÙŠØªÙ… Ø¹Ù† Ø·Ø±ÙŠÙ‚ API.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Schema (Ù…Ø®Ø·Ø· Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª)\n",
    "\n",
    "- **Schema** Ù‡Ùˆ ØªØ¹Ø±ÙŠÙ Ø£Ùˆ Ø®Ø±ÙŠØ·Ø© ØªÙˆØ¶Ø­ Ø´ÙƒÙ„ ÙˆØªØ±ÙƒÙŠØ¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n",
    "- Ø¨ÙŠØ­Ø¯Ø¯ Ø§Ù„Ø­Ù‚ÙˆÙ„ØŒ Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ù†ØµØŒ Ø±Ù‚Ù…ØŒ ØªØ§Ø±ÙŠØ®...)ØŒ ÙˆØ§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù„ÙŠ Ù„Ø§Ø²Ù… ØªØªØ¨Ø¹Ù‡Ø§ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n",
    "- Ø¨ÙŠØ³ØªØ®Ø¯Ù… ÙÙŠ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ APIsØŒ ÙˆÙ†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ©.\n",
    "- Ù…Ø«Ø§Ù„: Ù…Ø®Ø·Ø· JSON Schema ÙŠÙˆØ¶Ø­ Ø¥Ù† Ø­Ù‚Ù„ \"name\" Ù„Ø§Ø²Ù… ÙŠÙƒÙˆÙ† Ù†Øµ ÙˆØ­Ù‚Ù„ \"age\" Ø±Ù‚Ù….\n",
    "\n",
    "---\n",
    "\n",
    "## Ø§Ù„ÙØ±Ù‚ Ø¨Ø¨Ø³Ø§Ø·Ø©:\n",
    "\n",
    "- **API** Ù‡Ùˆ Ø§Ù„ÙˆØ³ÙŠØ· Ø§Ù„Ù„ÙŠ Ø¨ÙŠÙ† Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª.\n",
    "- **Schema** Ù‡Ùˆ Ø´ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù„ÙŠ Ø¨ØªÙ†ØªÙ‚Ù„ Ø¨ÙŠÙ† Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø£Ùˆ Ø¨ØªØ®Ø²Ù†.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce47349",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
