{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad432796",
   "metadata": {},
   "source": [
    "## Building A Chatbot\n",
    "\n",
    "<div dir='rtl'>\n",
    "\n",
    "في هذا الـ Notebook، سنتعلم كيفية تصميم وتنفيذ **شات بوت مدعوم بنموذج لغة كبير (LLM)**.  \n",
    "هذا الشات بوت سيكون قادرًا على إجراء محادثة وتذكر التفاعلات السابقة.\n",
    "\n",
    "---\n",
    "\n",
    "### ملاحظات مهمة:\n",
    "\n",
    "الشات بوت الذي سنبنيه هنا سيستخدم فقط نموذج اللغة لإجراء المحادثة.  \n",
    "ولكن هناك عدة مفاهيم متعلقة قد تود الاطلاع عليها أيضًا:\n",
    "\n",
    "- **Conversational RAG**: تمكين تجربة شات بوت تعتمد على مصدر خارجي للبيانات.\n",
    "- **Agents**: بناء شات بوت يمكنه اتخاذ إجراءات بناءً على المحادثة.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37fa3244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a57f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002CC280DE660>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002CC280DF7A0>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model = 'Gemma2-9b-It', groq_api_key= groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0257d0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Ahmed Sharaf, it's nice to meet you! \\n\\nThat's fantastic that you're an AI engineer. It's a really exciting field right now.  \\n\\nIs there anything I can help you with today? Perhaps you'd like to discuss some AI concepts, explore a specific project, or just chat about the latest developments in the field?\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 79, 'prompt_tokens': 23, 'total_tokens': 102, 'completion_time': 0.143636364, 'prompt_time': 0.002135306, 'queue_time': 0.157774412, 'total_time': 0.14577167}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--d2030072-089f-40b9-bc77-be34f345a767-0', usage_metadata={'input_tokens': 23, 'output_tokens': 79, 'total_tokens': 102})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "model.invoke([HumanMessage(content = 'Hi, My Name Is Ahmed Sharaf and I am a ai engineer')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5b7fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You told me your name is Ahmed Sharaf and that you are an AI engineer!  \\n\\nIs there anything else you'd like to tell me about yourself or your work? I'm eager to learn more. 😊 \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 81, 'total_tokens': 130, 'completion_time': 0.089090909, 'prompt_time': 0.004072432, 'queue_time': 0.16206112900000003, 'total_time': 0.093163341}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--f150d84a-d967-485c-9dbe-b683ba61ee0a-0', usage_metadata={'input_tokens': 81, 'output_tokens': 49, 'total_tokens': 130})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content='Hi, My Name Is Ahmed Sharaf and I am a ai engineer'),\n",
    "        AIMessage(content = \"Hello Ahmed Sharaf, it's nice to meet you! \\n\\nThat's fantastic that you're an AI engineer. It's a really exciting field right now\"), # هنا بقوله ان دا كان رد ال Ai \n",
    "        HumanMessage(content = \"Hey What's my name and what do i do ?\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# هنا قدر انه يفتكر المعلومات القديمه بس احنا ادينهاله يدويا بنفسنا\n",
    "# ازاى نعمل الكلام دا تلقائي"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6003c325",
   "metadata": {},
   "source": [
    "### Message History\n",
    "<div dir='rtl'>\n",
    "نستطيع استخدام **كلاس Message History** لتغليف النموذج وجعله **يحفظ الحالة (stateful)**.  \n",
    "هذا يعني أنه سيقوم بتتبع المدخلات والمخرجات الخاصة بالنموذج، ويخزنها في قاعدة بيانات أو مكان تخزين معين.\n",
    "datastore\n",
    "وبالتالي، عند التفاعل في المستقبل، يتم تحميل هذه الرسائل السابقة وتمريرها إلى الـ chain كجزء من الإدخال، مما يسمح للنموذج بفهم سياق المحادثة السابقة.\n",
    "\n",
    "---\n",
    "\n",
    "دعونا نرى كيفية استخدام هذا المفهوم عمليًا!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2181d03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# اول حاجه ازاى نفرق بين السيشنز المختلفه يعني بين المحادثات المختلفه\n",
    "# دا عن طريق داله بتوشف ال session id\n",
    "# ChatMessageHistory => دا فيه المحادثه كامله بالتاريخ\n",
    "\n",
    "# session_id type => string\n",
    "# -> return type\n",
    "store = dict()\n",
    "def get_session_history(session_id:str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# علشان اخليه يشغ الداله دي ف كل مره بنكلم الموديل ونخزن المحتوى هنا\n",
    "# بنستخدم RunnableWithMessageHistory\n",
    "# دا اللى بيكون ال pipline\n",
    "\n",
    "work_message_history = RunnableWithMessageHistory(model, get_session_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b85b166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Ahmed! It's great to meet you. \\n\\nBeing an AI engineer is such a cutting-edge field. What areas of AI are you most interested in or working on? I'm always curious to hear about the exciting projects people are involved in. 😊  \\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {'configurable': {'session_id':'chat1'}}\n",
    "response = work_message_history.invoke(\n",
    "    [HumanMessage(content='Hi, My Name Is Ahmed Sharaf and I am a ai engineer')],\n",
    "    config = config,\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Ahmed Sharaf.  😊 \\n\\nI remembered it from our first introduction!  \\n\\n\\n\\n\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 174, 'total_tokens': 198, 'completion_time': 0.043636364, 'prompt_time': 0.007277485, 'queue_time': 0.1595678, 'total_time': 0.050913849}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--cd205b2b-42f2-4d94-bb2e-90e77aa37a7b-0', usage_metadata={'input_tokens': 174, 'output_tokens': 24, 'total_tokens': 198})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "work_message_history.invoke(\n",
    "    [HumanMessage(content='What is my name?')],\n",
    "    config = config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6adbeea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As a large language model, I have no memory of past conversations or access to personal information. Therefore, I don't know your name.\\n\\nIf you'd like to tell me your name, I'd be happy to know!\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 52, 'prompt_tokens': 14, 'total_tokens': 66, 'completion_time': 0.094545455, 'prompt_time': 0.001914866, 'queue_time': 0.16107137500000002, 'total_time': 0.096460321}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--74d83f09-0b61-47ad-a72e-b16d05590bda-0', usage_metadata={'input_tokens': 14, 'output_tokens': 52, 'total_tokens': 66})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we change the config (session id) the model can't remeber me\n",
    "config1 = {'configurable': {'session_id': 'chat2'}}\n",
    "work_message_history.invoke(\n",
    "    [HumanMessage(content='What is my name?')],\n",
    "    config = config1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34e2ec6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Ali, it's nice to meet you! 😊\\n\\nIs there anything I can help you with today?\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 77, 'total_tokens': 103, 'completion_time': 0.047272727, 'prompt_time': 0.003847522, 'queue_time': 0.159495923, 'total_time': 0.051120249}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--4c7165a5-62d9-4350-954a-ed857571f832-0', usage_metadata={'input_tokens': 77, 'output_tokens': 26, 'total_tokens': 103})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "work_message_history.invoke(\n",
    "    [HumanMessage(content='My Name Ali')],\n",
    "    config = config1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47bc362e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Ali.  \\n\\nI remember that you told me earlier!  😄  How can I help you today?\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 115, 'total_tokens': 143, 'completion_time': 0.050909091, 'prompt_time': 0.005832969, 'queue_time': 0.160230962, 'total_time': 0.05674206}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--7d8aea32-c238-4658-8edf-442bdc6437c2-0', usage_metadata={'input_tokens': 115, 'output_tokens': 28, 'total_tokens': 143})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "work_message_history.invoke(\n",
    "    [HumanMessage(content='whats my name?')],\n",
    "    config = config1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26efe1c9",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    "# ✅ ملخص وظائف:\n",
    "| الكلاس | وظيفته | علاقته بالذاكرة (Memory) |\n",
    "|---------|---------|-------------------------|\n",
    "| **ChatMessageHistory** | كلاس مسؤول عن تخزين واسترجاع تاريخ الرسائل (User + AI) داخل المحادثة | بيحتفظ بالرسائل في الذاكرة (in-memory) أو ممكن يتخزن في قواعد بيانات زي Redis |\n",
    "| **BaseChatMessageHistory** | كلاس مجرد (Abstract Base Class) بيحدد الواجهة الأساسية لأي Message History | مش بيخزن حاجة بنفسه، لكن بيحدد القواعد (interface) اللي ChatMessageHistory وغيره لازم يلتزموا بيها |\n",
    "| **RunnableWithMessageHistory** | Wrapper بيربط الـ Chain أو Runnable بتاريخ المحادثة تلقائي | بياخد الرسائل من الـ history، يمررها للموديل، وبعد الرد بيحدث التاريخ بدون ما انت تتدخل |\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ باختصار:\n",
    "- **ChatMessageHistory** = التخزين الفعلي للرسائل في الذاكرة.\n",
    "- **BaseChatMessageHistory** = القالب اللي كل أنواع الـ history لازم تمشي عليه.\n",
    "- **RunnableWithMessageHistory** = الكائن اللي بيربط الـ Chain مع التاريخ تلقائي ويخليك تنسى التفاصيل.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf7a7c2",
   "metadata": {},
   "source": [
    "### Prompt templates\n",
    "Prompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let's now make that a bit more complicated. First, let's add in a system message with some custom instructions (but still taking messages as input). Next, we'll add in more input besides just the messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1ee93f",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    "# ✅ ما هو MessagesPlaceholder في LangChain؟\n",
    "\n",
    "### 🟡 التعريف:\n",
    "- `MessagesPlaceholder` هو **مكون (Component)** بيمثل **مكان مخصص** داخل الـ Prompt Template.\n",
    "- وظيفته إنه يكون مكان (Placeholder) نقدر نحط فيه ** الرسائل (Message)** بعدين.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ ليه بنستخدمه؟\n",
    "| السبب | الشرح |\n",
    "|--------|-------|\n",
    "| **عشان ندخل التاريخ في البرومبت بسهولة** | بدل ما نكتب الرسائل يدوي جوا البرومبت، بنحط MessagesPlaceholder وهو بيستقبلها وقت التنفيذ. |\n",
    "| **يحافظ على ترتيب الرسائل (User + AI)** | بيضيف الرسائل بالترتيب الصح في الـ input بتاع الموديل. |\n",
    "| **بيشتغل مع RunnableWithMessageHistory** | لما تستخدمهم مع بعض، الـ Placeholder ده بيتملّى تلقائي بالتاريخ اللي محفوظ. |\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98360a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', 'You Are a helpful assistant. Answer all the question to the nest of your ability'),\n",
    "        MessagesPlaceholder(variable_name = 'messages')\n",
    "    ]\n",
    ")\n",
    "chain = prompt|model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a885631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Ahmed Sharaf, it's nice to meet you!\\n\\nI'm ready to answer your questions to the best of my ability.  What can I help you with today? 😊  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 32, 'total_tokens': 76, 'completion_time': 0.08, 'prompt_time': 0.002365776, 'queue_time': 0.16124811700000002, 'total_time': 0.082365776}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--cca7a0b7-aaeb-40da-9f90-c99cf10af8f7-0', usage_metadata={'input_tokens': 32, 'output_tokens': 44, 'total_tokens': 76})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'messages':[HumanMessage(content = 'Hi My Name Is Ahmed Sharaf')]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9bda7176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How Invoke with messages history\n",
    "with_message_history = RunnableWithMessageHistory(chain, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3da6ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config3 = {'configurable': {'session_id': \"chat4\"}}\n",
    "respone = with_message_history.invoke(\n",
    "    [HumanMessage(content = 'Hi My Name Is Ahmed Sharaf')],\n",
    "    config=config3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4fed3742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Ahmed Sharaf, it's nice to meet you! \\n\\nI'm happy to help with any questions you have.  \\n\\nWhat can I do for you today? 😊  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 32, 'total_tokens': 75, 'completion_time': 0.078181818, 'prompt_time': 0.002337925, 'queue_time': 0.161324813, 'total_time': 0.080519743}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--ff84c9a8-ca5e-4aae-812c-d7408f174e19-0', usage_metadata={'input_tokens': 32, 'output_tokens': 43, 'total_tokens': 75})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0189dc46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Ahmed! It's great to meet you. \\n\\nBeing an AI engineer is such a cutting-edge field. What areas of AI are you most interested in or working on? I'm always curious to hear about the exciting projects people are involved in. 😊  \\n\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respone = with_message_history.invoke(\n",
    "    [HumanMessage(content = \"what's my name?\")],\n",
    "    config=config3\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ddb5e973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets add multiple variable\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', 'You are a helpful assistant. Answer all Question to the best of your ability in {language}'),\n",
    "        MessagesPlaceholder(variable_name='messages')\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1da33617",
   "metadata": {},
   "outputs": [],
   "source": [
    "respone = chain.invoke({'messages': [HumanMessage(content=\"Hi my name is ahmed\")], 'language': 'arabic'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1d941840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='مرحباً أحمد، يسعدني التعرّف عليك! \\n\\nكيف يمكنني مساعدتك اليوم؟  😊 \\n\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 32, 'total_tokens': 62, 'completion_time': 0.054545455, 'prompt_time': 0.002398085, 'queue_time': 0.161231276, 'total_time': 0.05694354}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--aadfd6c3-6f9b-4fa6-abce-5df44468f1c4-0', usage_metadata={'input_tokens': 32, 'output_tokens': 30, 'total_tokens': 62})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a155b6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain, \n",
    "                                                  get_session_history,\n",
    "                                                  input_messages_key='messages')\n",
    "\n",
    "# input_messages_key => هنا بتعرفه على ال messages اللى هتدخله \n",
    "# لاحظ اى متعير ف ال prompt بندخل قيمته هنا ب dictionary \n",
    "# key => اسم المتغير ف ال prompt\n",
    "# value => القيمه اللى هيعوض بيها مكانه ف ال prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf6017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"مرحبا أحمد، يسعدني مساعدتك!  \\n\\n(Hello Ahmed, I'm happy to help you! )\\n\\nما الذي يمكنني أن أفعل لك؟ \\n\\n(What can I do for you?) \\n\\n\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_5 = {'configurable': {'session_id': 'chat10'}}\n",
    "response = with_message_history.invoke(\n",
    "    {'messages': [HumanMessage(content='Hi, I am ahmed')],\n",
    "     'language': 'Arabic'},\n",
    "    config=config_5\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3c713220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'اسمك أحمد. 😊 \\n\\n(Your name is Ahmed. 😊) \\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {'messages': [HumanMessage(content='what is my name?')],\n",
    "     'language': 'Arabic'},\n",
    "    config=config_5\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ff4d0f",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    "# ✅ إدارة تاريخ المحادثة (Managing Conversation History)\n",
    "\n",
    "### 🟡 المشكلة:\n",
    "- كل مرة بنتكلم مع الشات بوت، الرسائل (user + AI) بتضاف لتاريخ المحادثة.\n",
    "- مع الوقت، التاريخ بيكبر جدًا وممكن:\n",
    "  - **يتجاوز الـ Context Window** للنموذج (يعني الموديل مش هيقدر يشوف كل الرسائل).\n",
    "  - يبطّأ الأداء ويستهلك Tokens زيادة.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ الحل: استخدام **Trimmer**\n",
    "- هو أداة (Helper Function) بتساعد في **تقليل حجم الرسائل** اللي بنبعتها للموديل.\n",
    "- الهدف إننا نفضل محافظين على **أهم الرسائل** بس، والباقي بنرميه أو بنقصه.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ وظيفة `trim_messages`:\n",
    "| الخاصية | شرحها |\n",
    "|----------|-------|\n",
    "| **تحديد عدد الـ Tokens اللي عايزين نحافظ عليهم** | بنقول له مثلًا: \"ما تتعداش 2000 token\" |\n",
    "| **إبقاء رسالة الـ System دايمًا** | عشان توجه النموذج دايمًا مهما قصينا من التاريخ. |\n",
    "| **السماح بالـ Partial Messages** | يعني ممكن نقص جزء من رسالة بدل ما نمسحها كلها. |\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ ليه مهم؟\n",
    "- بيحميك من **Overflow** في الـ Context.\n",
    "- بيحافظ على **السياق الأهم** (recent messages + system prompt).\n",
    "- بيخلي الشات بوت **شغال بكفاءة** حتى مع جلسات طويلة.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a46f55ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens = 45,\n",
    "    strategy = 'last', # الطريقه اللى هيعرف بيها ان دي اهم جزء من الرسائل فيحافظ عليه ويرمي الباقي\n",
    "    token_counter = model,\n",
    "    include_system = True,\n",
    "    allow_partial = False,\n",
    "    start_on = 'human'\n",
    ")\n",
    "\n",
    "# بنينا محادثه بين انسان وذكاء اصطناعى\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d3f307",
   "metadata": {},
   "source": [
    "## ✅ Trimmer Parameters\n",
    "\n",
    "<div dir=\"rtl\">\n",
    "\n",
    "| البراميتر               | الوظيفة                                    | التفسير                                                   |\n",
    "|------------------------|--------------------------------------------|-----------------------------------------------------------|\n",
    "| `max_tokens = 45`       | أقصى عدد من الـ Tokens بعد التريمنج        | الموديل هيشوف رسائل مجموعهم لا يتجاوز 45 Token فقط         |\n",
    "| `strategy = 'last'`     | استراتيجية اختيار الرسائل                 | هيحتفظ بآخر الرسائل (الأحدث) ويقص القديمة                  |\n",
    "| `token_counter = model` | أداة لحساب عدد الـ Tokens في الرسائل       | بيستخدم الموديل نفسه لحساب حجم الرسائل بالتوكنز           |\n",
    "| `include_system = True` | هل نحافظ على system message مهما حصل؟     | نعم، هتفضل رسالة الـ system موجودة حتى بعد القص           |\n",
    "| `allow_partial = False` | السماح بقص جزء من رسالة؟                  | لأ، إما الرسالة تفضل كاملة أو تتشال خالص                   |\n",
    "| `start_on = 'human'`    | منين نبدأ التريمنج؟                       | هيبدأ من أول رسالة بشرية (Human Message)                   |\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "16cb99a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'أنا مساعد افتراضي ولا أعرف تفضيلاتك الشخصية، بما في ذلك نوع الآيس كريم الذي تحبه.  \\n\\nهل يمكنك مشاركة نوع الآيس كريم الذي تفضله؟ 😊 \\n\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how do pass trimmer in chain ?\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter('messages')|trimmer)\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "response = chain.invoke(\n",
    "    {\n",
    "    'messages': messages + [HumanMessage(content = 'What ice cream do i like')],\n",
    "    'language': 'Arabic'\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3895f4eb",
   "metadata": {},
   "source": [
    "-  هنا مفتكرش علشان ال trimmer مخدش الجزء دا ف المهم اللى بعته له "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d33a4c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'سألتني 2 + 2  \\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "    'messages': messages + [HumanMessage(content = 'What math problem did i ask')],\n",
    "    'language': 'Arabic'\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bfe22843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I don't have memory of past conversations or personal information about you.  So I don't know your name. Would you like to tell me? 😊  \\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets Wrap this in the Message History\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key='messages'\n",
    ")\n",
    "config = {'configurable': {'session_id': 'chat100'}}\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        'messages': messages + [HumanMessage(content = \"what's my name\")],\n",
    "        'language': 'English'\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf90f58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
