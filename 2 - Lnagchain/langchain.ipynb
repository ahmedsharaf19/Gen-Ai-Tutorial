{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3376b491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a8edea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# For Langchain-smith to tracing Project and Monitoring model\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_PROJECT'] = 'gentest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4084fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'gemma3:1b', 'format': None, 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': None, 'num_thread': None, 'num_predict': None, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': None, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None, 'raw': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmed\\AppData\\Local\\Temp\\ipykernel_4740\\1499390201.py:3: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model = 'gemma3:1b')\n"
     ]
    }
   ],
   "source": [
    "# load llm model\n",
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model = 'gemma3:1b')\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9671fea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input and get response from llm\n",
    "result = llm.invoke(\"What is generative ai?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8000a9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's break down generative AI. It's a really fascinating and rapidly evolving field, but here's a breakdown of what it is, how it works, and why it's so impactful:\n",
      "\n",
      "**1. What is Generative AI?**\n",
      "\n",
      "At its core, generative AI refers to a type of artificial intelligence that can **create new content**.  Instead of just analyzing existing data and finding patterns, it *generates* original content – text, images, audio, video, code, and more.  Think of it as an AI that can *imagine* and then bring that imagination to life.\n",
      "\n",
      "**2. How Does it Work?**\n",
      "\n",
      "Here's a simplified explanation of the common techniques used:\n",
      "\n",
      "* **Machine Learning (ML):** Generative AI systems are typically built using machine learning.  They're trained on massive datasets of existing content.\n",
      "* **Neural Networks:**  The most popular approach uses neural networks – complex mathematical models inspired by the human brain.  Specifically, **Generative Adversarial Networks (GANs)** and **Large Language Models (LLMs)** are key.\n",
      "* **GANs (Generative Adversarial Networks):** These involve two networks competing against each other. One network (the generator) creates content, and the other network (the discriminator) tries to distinguish between the generated content and real data.  They learn from each other, constantly improving until the generator can produce remarkably realistic content.\n",
      "* **LLMs (Large Language Models):** These models, like GPT-3, GPT-4, or Llama, are trained on enormous amounts of text data. They learn to predict the next word in a sequence, allowing them to generate text in a variety of styles, answer questions, translate languages, and much more.\n",
      "\n",
      "**3. Key Types of Generative AI**\n",
      "\n",
      "* **Image Generation:**  (e.g., DALL-E, Midjourney, Stable Diffusion) – Create images from text prompts. You can specify objects, styles, and even moods.\n",
      "* **Text Generation:** (e.g., ChatGPT, Bard, Claude) – Write articles, poems, code, summaries, scripts, emails, and more.\n",
      "* **Music Generation:** (e.g., Amper Music, Jukebox) – Create original music pieces in various genres.\n",
      "* **Video Generation:** (e.g., RunwayML, Synthesia) – Create short videos from text descriptions or other input.\n",
      "* **3D Model Generation:** (e.g., Adobe Firefly, Craiyon) – Create 3D models of objects or characters.\n",
      "\n",
      "**4.  Examples You Might Have Heard Of**\n",
      "\n",
      "* **ChatGPT:**  A chatbot that can answer your questions, generate text, and even write different kinds of creative content.\n",
      "* **DALL-E 2/3:** Creates images from text descriptions.\n",
      "* **Midjourney:**  Known for its artistic and surreal image generation.\n",
      "* **Stable Diffusion:** Another powerful open-source image generation tool.\n",
      "\n",
      "\n",
      "**5.  Key Capabilities & Uses**\n",
      "\n",
      "* **Content Creation:** Speeding up content creation across various fields.\n",
      "* **Art & Design:** Creating unique visual assets.\n",
      "* **Marketing & Advertising:** Generating engaging marketing materials.\n",
      "* **Product Design:**  Assisting in the design process.\n",
      "* **Education:** Personalized learning materials.\n",
      "* **Research:**  Accelerating data analysis and discovery.\n",
      "\n",
      "\n",
      "**Important Note:** Generative AI is a rapidly evolving field.  It's important to be aware of the ethical considerations and potential biases that these models can inherit from their training data.\n",
      "\n",
      "\n",
      "\n",
      "**Resources for Further Learning:**\n",
      "\n",
      "* **OpenAI:** [https://openai.com/blog/generative-ai](https://openai.com/blog/generative-ai)\n",
      "* **Google AI:** [https://ai.googleblog.com/](https://ai.googleblog.com/)\n",
      "* **DeepLearning.AI:** [https://www.deeplearning.ai/](https://www.deeplearning.ai/)\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "To help me tailor my explanation further, could you tell me:\n",
      "\n",
      "*   **What specifically are you interested in learning about generative AI?** (e.g., its applications, the technology behind it, the ethical concerns?)\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "008a17d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55020e2",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "\n",
    "## ✅ ما هو الـ **Prompt Template**؟\n",
    "الـ **Prompt Template** هو قالب يستخدم لتحديد **إزاي عايز الـ LLM يتصرف** و**إيه الدور** اللي هتلعبه. الهدف من الـ Template هو توجيه الـ LLM للقيام بوظائف معينة بناءً على الدور الذي تعطيه له.\n",
    "\n",
    "### إزاي ده بيشتغل؟\n",
    "الـ **Prompt Template** زي **دليل إرشادي** بيقول للموديل:\n",
    "- **إزاي يتصرف؟** \n",
    "- **إيه الدور اللي هيلعبه؟** \n",
    "\n",
    "ممكن تعطيه **دور مساعد** زي إنك تطلب منه أنه يكون **مساعد برمجي** أو **مترجم** أو حتى **مستشار**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce33245f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You Are An Expert Ai Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chatprompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You Are An Expert Ai Engineer. Provide me answer based on the question\"),\n",
    "        ('user', \"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d105c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt|llm  # | => بنقوله combine الاتنين دول مع بعض ف سلسله متصله\n",
    "# Steps \n",
    "# input go to template\n",
    "# then template go to llm\n",
    "# then llm give response\n",
    "response = chain.invoke({'input': 'Can You Tell me About Langsmith?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d26460d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Okay, let’s dive into Langsmith! As an AI engineer, I can provide you with a comprehensive overview of Langsmith, outlining its purpose, key features, and why it’s gaining popularity. Here’s a breakdown, broken down into key areas:\\n\\n**1. What is Langsmith?**\\n\\nLangsmith is a **privacy-focused, AI-powered document management and collaboration platform** designed specifically for legal professionals. It’s built to address the growing challenges of securely handling sensitive legal documents – specifically, those that require strict confidentiality.  Think of it as a digital vault for legal documents with built-in AI-driven security and workflow automation.\\n\\n**2. Core Purpose & Key Features – What Makes it Unique?**\\n\\n* **Privacy by Design:** This is the *defining* feature. Langsmith is built with privacy as its primary concern. It's fundamentally designed to minimize data collection and maintain client confidentiality.  It leverages end-to-end encryption (E2EE) and data minimization principles.\\n* **AI-Powered Document Processing:** Langsmith uses AI, specifically Natural Language Processing (NLP), to automate tasks like:\\n    * **Document Upload & Analysis:**  It analyzes documents *before* you upload them – detecting sensitive information (e.g., client lists, confidential agreements, personally identifiable information – PII) and flags it for review.\\n    * **Automated Workflow:** It automates complex legal workflows - from initial document review to e-signature, compliance checks, and even task assignment.\\n    * **Predictive Insights:** It can suggest review actions based on the document's content, speeding up the process.\\n* **Secure Document Storage & Retrieval:**  It stores documents securely within its own private database, offering high-level security.\\n* **Role-Based Access Control (RBAC):**  Administrators can define granular permissions for different users and roles, ensuring only authorized personnel can access specific documents.\\n* **Version Control:**  Tracks document versions, allowing you to revert to previous states if needed.\\n* **Audit Trails:** Provides a complete audit trail of all document activity, critical for compliance.\\n* **Integration Capabilities:**  Offers integrations with popular legal software platforms like Clio, MyCase, and more.\\n* **Compliance Support:**  Built-in tools to assist with GDPR, CCPA, and other regulatory compliance requirements.\\n\\n\\n**3. Who is Langsmith For?**\\n\\n* **Law Firms:**  Managing client data, contracts, and legal matters effectively.\\n* **Corporate Legal Departments:** Securely storing and managing sensitive documents related to compliance, intellectual property, and contracts.\\n* **Individual Lawyers:**  Providing a secure and efficient platform for handling confidential client documents.\\n* **LegalTech Startups:** Building privacy-focused legal solutions.\\n\\n\\n**4.  Why is Langsmith Becoming Popular?**\\n\\n* **Growing Regulatory Pressure:** Increasing legal regulations around data privacy demand secure storage and management of sensitive information.\\n* **User Demand:** There’s a growing awareness and demand for privacy-focused solutions within the legal industry.\\n* **AI Advancement:** The advancements in AI and NLP are making document processing significantly faster and more accurate.\\n* **Strong Focus on Security:** Langsmith has consistently prioritized security as a core value.\\n\\n\\n**5.  Cost & Pricing**\\n\\nLangsmith operates on a subscription-based model.  Pricing is tiered, offering different levels of storage, features, and support. They currently have a free trial – offering a good way to experience the platform.\\n\\n**Resources to Explore Further:**\\n\\n* **Langsmith Website:** [https://langsmith.app/](https://langsmith.app/) - This is the best starting point.\\n* **Langsmith Blog:** [https://langsmith.app/blog/](https://langsmith.app/blog/) -  Learn about new features and case studies.\\n* **Langsmith Documentation:** [https://langsmith.app/docs/](https://langsmith.app/docs/) - Explore the features and options within the platform.\\n\\n\\n**To help me tailor my answer even further, could you tell me:**\\n\\n*   **What specifically are you interested in learning about Langsmith?** (e.g., are you interested in its features, security, or integration capabilities?)\\n*   **What is your background or level of technical expertise regarding legal technology?** (This will help me explain it in a way that is appropriate for your understanding.)\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6d9072c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let’s dive into Langsmith. As an AI engineer, I can provide you with a comprehensive overview. Here’s a breakdown of what Langsmith is, what it does, its key features, and why it’s become so popular:\n",
      "\n",
      "**What is Langsmith?**\n",
      "\n",
      "Langsmith is a **privacy-focused, asynchronous legal communication platform** built for lawyers and legal professionals. It’s designed to streamline legal document exchange, significantly reducing reliance on email and potentially risky sharing methods. Think of it as a secure, digital \"notebook\" for legal conversations.\n",
      "\n",
      "**Key Features & Functionality:**\n",
      "\n",
      "* **Asynchronous Communication:** This is *the* core differentiator. Langsmith moves beyond immediate email replies.  Clients can leave responses to questions within the platform, and the system handles drafting replies automatically. This minimizes friction and allows for more thoughtful, deliberate communication.\n",
      "* **Document Sharing & Collaboration:**  Langsmith facilitates the secure sharing of documents (PDFs, Word, etc.) with intelligent, context-aware replies.  It automatically creates replies based on the content of the initial document.\n",
      "* **Smart Replies & Templates:**  Langsmith provides a library of pre-built templates and smart replies (like \"Confirm this is a response to my request\") that lawyers can adapt to their specific needs. This speeds up the process.\n",
      "* **Contextual Awareness:** This is a *huge* improvement. Langsmith analyzes the document and the conversation to understand the key points and relevant context. It then crafts replies that directly address these points, saving lawyers time and ensuring clarity.  It also uses semantic understanding to suggest replies.\n",
      "* **Version Control & Audit Trails:**  Every message, response, and document change is logged, creating a complete audit trail. This is critical for legal compliance and dispute resolution.\n",
      "* **Security & Privacy:**  Langsmith emphasizes security through end-to-end encryption, strict access controls, and a commitment to data privacy. It’s built on a robust, privacy-first architecture.\n",
      "* **Integration with Legal Platforms:** Langsmith integrates with popular legal platforms like Clio, MyCase, and Ava, streamlining workflows.\n",
      "* **AI-Powered Assistance (Early Stages):** While not fully mature, Langsmith is incorporating AI to help with document analysis, redaction, and even initial draft generation – boosting efficiency.\n",
      "\n",
      "\n",
      "**Why is Langsmith Becoming Popular?**\n",
      "\n",
      "* **Addressing the Legal Workload:**  Legal professionals are facing increasing pressure to manage complex tasks and maintain strong relationships with clients. Langsmith offers a solution to streamline communication.\n",
      "* **Reducing Risk:**  Email can be easily misinterpreted or shared with the wrong people. Langsmith minimizes these risks through secure, asynchronous communication.\n",
      "* **Improved Efficiency:** Automation of replies significantly cuts down on time spent on routine tasks, allowing lawyers to focus on more strategic work.\n",
      "* **Enhanced Client Experience:**  Better communication leads to better client relationships.\n",
      "\n",
      "\n",
      "**Cost Structure:**\n",
      "\n",
      "Langsmith is offered as part of a **paid subscription service**.  The pricing varies depending on the number of users and features required. Expect to pay a monthly fee for the service.\n",
      "\n",
      "\n",
      "**Resources to Learn More:**\n",
      "\n",
      "* **Langsmith Website:** [https://langsmith.app/](https://langsmith.app/) -  This is the primary source for all information.\n",
      "* **Langsmith Blog:** [https://langsmith.app/blog/](https://langsmith.app/blog/) -  Stay updated on new features and best practices.\n",
      "* **Langsmith Case Studies:** [https://langsmith.app/case-studies/](https://langsmith.app/case-studies/) - See real-world examples of how Langsmith is being used.\n",
      "\n",
      "**In short, Langsmith is a sophisticated tool designed to revolutionize the way legal professionals communicate and collaborate.**\n",
      "\n",
      "---\n",
      "\n",
      "**To help me give you even more tailored information, could you tell me:**\n",
      "\n",
      "*   **What specifically are you interested in knowing about Langsmith?** (e.g., are you looking for a comparison to other platforms, understanding its technical aspects, or exploring specific use cases?)\n"
     ]
    }
   ],
   "source": [
    "## stroutput parser\n",
    "## دا بنتحكم ف طريقة عرض ال response من ال llm بس\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt|llm|output_parser\n",
    "\n",
    "response = chain.invoke({'input': 'Can You Tell me About Langsmith?'})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2c86c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Ingestion--From the website we need to scrape the data\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56a92f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x25c9d5312e0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader=WebBaseLoader(\"https://docs.smith.langchain.com/administration/tutorials/manage_spend\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb25ad36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nOptimize tracing spend on LangSmith | 🦜️🛠️ LangSmith\\n\\n\\n\\n\\n\\n\\nSkip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith. Join our team!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationTutorialsOptimize tracing spend on LangSmithHow-to GuidesSetupConceptual GuideSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceChangelogCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceAdministrationTutorialsOptimize tracing spend on LangSmithOn this pageOptimize tracing spend on LangSmith\\nRecommended ReadingBefore diving into this content, it might be helpful to read the following:\\nData Retention Conceptual Docs\\nUsage Limiting Conceptual Docs\\n\\nnoteSome of the features mentioned in this guide are not currently available in Enterprise plan due to its\\ncustom nature of billing. If you are on Enterprise plan and have questions about cost optimization,\\nplease reach out to your sales rep or support@langchain.dev.\\nThis tutorial walks through optimizing your spend on LangSmith. In it, we will learn how to optimize existing spend\\nand prevent future overspend on a realistic real-world example. We will use an existing LangSmith organization with high usage.\\nConcepts can be transferred to your own organization.\\nProblem Setup\\u200b\\nIn this tutorial, we take an existing organization that has three workspaces, one for each deployment stage\\n(Dev, Staging, and Prod):\\n\\nUnderstand your current usage\\u200b\\nThe first step of any optimization process is to understand current usage. LangSmith gives two ways to do this: Usage Graph\\nand Invoices.\\nUsage Graph\\u200b\\nThe usage graph lets us examine how much of each usage based pricing metric we have consumed lately. It does not directly show\\nspend (which we will see later on our draft invoice).\\nWe can navigate to the Usage Graph under Settings -> Usage and Billing -> Usage Graph.\\n\\nWe see in the graph above that there are two usage metrics that LangSmith charges for:\\n\\nLangSmith Traces (Base Charge)\\nLangSmith Traces (Extended Data Retention Upgrades).\\n\\nThe first metric tracks all traces that you send to LangSmith. The second tracks all traces that also have our Extended 400 Day Data Retention.\\nFor more details, see our data retention conceptual docs. Notice that these graphs look\\nidentical, which will come into play later in the tutorial.\\nLangSmith Traces usage is measured per workspace, because workspaces often represent development environments (as in our example),\\nor teams within an organization. As a LangSmith administrator, we want to understand spend granularly per each of these units. In\\nthis case where we just want to cut spend, we can focus on the environment responsible for the majority of costs first for the greatest savings.\\nnoteLangSmith\\'s Usage Graph and Invoice use the term tenant_id to refer to a workspace ID. They are interchangeable.\\nIn the above image, the vast majority of usage is in the workspace with ID c27dd32c-7c80-4e8c-acde-bfcb67a90ab2. We can\\ngo to Settings -> Workspaces, and hover our mouse over the Workspace ID button to find the one with a matching ID. In\\nthis case, it\\'s the Prod workspace:\\n\\nInvoices\\u200b\\nWe understand what usage looks like in terms of traces, but we now need to translate that into spend. To do so,\\nwe head to the Invoices tab. The first invoice that will appear on screen is a draft of your current month\\'s\\ninvoice, which shows your running spend thus far this month.\\n\\nIn the above GIF, we see that the charges for LangSmith Traces are broken up by \"tenant_id\" (i.e. Workspace ID), meaning we can track tracing spend\\non each of our workspaces. In the first few days of June, the vast majority of the total spend of ~$2,000 is in our production\\nworkspace. Further, the majority of spend in that workspace was on extended data retention trace upgrades.\\nThese upgrades occur for two reasons:\\n\\nYou use extended data retention tracing, meaning that, by default, your traces are retained for 400 days\\nYou use base data retention tracing, and use a feature that automatically extends the data retention of a trace (see our Auto-Upgrade conceptual docs)\\n\\nGiven that the number of total traces per day is equal to the number of extended retention traces per day, it\\'s most likely the\\ncase that this org is using extended data retention tracing everywhere. As such, we start by optimizing our retention settings.\\nOptimization 1: manage data retention\\u200b\\nLangSmith charges differently based on a trace\\'s data retention (see our data retention conceptual docs),\\nwhere short-lived traces are an order of magnitude less expensive than ones that last for a long time. In this optimization, we will\\nshow how to get optimal settings for data retention without sacrificing historical observability, and\\nshow the effect it has on our bill.\\nChange org level retention defaults for new projects\\u200b\\nWe navigate to the Usage configuration tab, and look at our organization level retention settings. Modifying this setting affects all new projects that are\\ncreated going forward in all workspaces in our org.\\nnoteFor backwards compatibility, older organizations may have this defaulted to Extended. Organizations created after June 3rd\\nhave this defaulted to Base.\\n\\nChange project level retention defaults\\u200b\\nOur existing projects have not changed their data retention settings, so we can change these on the individual project pages.\\nWe navigate to Projects -> <your project name>, click the data retention drop down, and modify it to base retention. As\\nwith the organization level setting, this will only affect retention (and pricing) for traces going forward.\\n\\nKeep around a percentage of traces for extended data retention\\u200b\\nWe may not want all our traces to expire after 14 days if we care about historical debugging. As such, we can take advantage\\nof LangSmith\\'s built in ability to do server side sampling for extended data retention.\\nChoosing the right percentage of runs to sample depends on your use case. We will arbitrarily pick 10% of runs here, but will\\nleave it to the user to find the right value that balances collecting rare events and cost constraints.\\nLangSmith automatically upgrades the data retention for any trace that matches a run rule in our automations product (see our run rules docs). On the\\nprojects page, click Rules -> Add Rule, and configure the rule as follows:\\n\\nRun rules match on runs rather than traces. Runs are single units of work within an LLM application\\'s API handling. Traces\\nare end to end API calls (learn more about tracing concepts in LangSmith). This means a trace can\\nbe thought of as a tree of runs making up an API call. When a run rule matches any run within a trace, the trace\\'s full run tree\\nupgrades to be retained for 400 days.\\nTherefore, to make sure we have the proper sampling rate on traces, we take advantage of the\\nfiltering functionality of run rules.\\nWe add add a filter condition to only match the \"root\" run in the run tree. This is distinct per trace, so our 10% sampling\\nwill upgrade 10% of traces, rather 10% of runs, which could correspond to more than 10% of traces. If desired, we can optionally add\\nany other filtering conditions required (e.g. specific tags/metadata attached to our traces) for more pointed data retention\\nextension. For the sake of this tutorial, we will stick with the simplest condition, and leave more advanced filtering as an\\nexercise to the user.\\nnoteIf you want to keep a subset of traces for longer than 400 days for data collection purposes, you can create another run\\nrule that sends some runs to a dataset of your choosing. A dataset allows you to store the trace inputs and outputs (e.g., as a key-value dataset),\\nand will persist indefinitely, even after the trace gets deleted.\\nSee results after 7 days\\u200b\\nWhile the total amount of traces per day stayed the same, the extended data retention traces was cut heavily.\\n\\nThis translates to the invoice, where we\\'ve only spent about $900 in the last 7 days, as opposed to $2,000 in the previous 4.\\nThat\\'s a cost reduction of nearly 75% per day!\\n\\nOptimization 2: limit usage\\u200b\\nIn the previous section, we managed data retention settings to optimize existing spend. In this section, we will\\nuse usage limits to prevent future overspend.\\nLangSmith has two usage limits: total traces and extended retention traces. These correspond to the two metrics we\\'ve\\nbeen tracking on our usage graph. We can use these in tandem to have granular control over spend.\\nTo set limits, we navigate back to Settings -> Usage and Billing -> Usage configuration. There is a table at the\\nbottom of the page that lets you set usage limits per workspace. For each workspace, the two limits appear, along\\nwith a cost estimate:\\n\\nLets start by setting limits on our production usage, since that is where the majority of spend comes from.\\nSetting a good total traces limit\\u200b\\nPicking the right \"total traces\" limit depends on the expected load of traces that you will send to LangSmith. You should\\nclearly think about your assumptions before setting a limit.\\nFor example:\\n\\nCurrent Load: Our gen AI application is called between 1.2-1.5 times per second, and each API request has a trace associated with it,\\nmeaning we log around 100,000-130,000 traces per day\\nExpected Growth in Load: We expect to double in size in the near future.\\n\\nFrom these assumptions, we can do a quick back-of-the-envelope calculation to get a good limit of:\\nlimit = current_load_per_day * expected_growth * days/month      = 130,000 * 2 * 30      = 7,800,000 traces / month\\nWe click on the edit icon on the right side of the table for our Prod row, and can enter this limit as follows:\\n\\nnoteWhen set without the extended data retention traces limit, the maximum cost estimator assumes that all traces are using extended data retention.\\nCutting maximum spend with an extended data retention limit\\u200b\\nIf we are not a big enterprise, we may shudder at the ~$40k per month bill.\\nWe saw from Optimization 1 that the easiest way to cut cost was through managing data retention.\\nThe same can be said for limits. If we only want to keep ~10% of traces to be around more than 14 days, we can set a limit on the maximum\\nhigh retention traces we can keep. That would be .10 * 7,800,000 = 780,000.\\n\\nAs we can see, the maximum cost is cut from ~40k per month to ~7.5k per month, because we no longer allow as many expensive\\ndata retention upgrades. This lets us be confident that new users on the platform will not accidentally cause cost to balloon.\\nnoteThe extended data retention limit can cause features other than traces to stop working once reached. If you plan to\\nuse this feature, please read more about its functionality here.\\nSet dev/staging limits and view total spent limit across workspaces\\u200b\\nFollowing the same logic for our dev and staging environments, we set limits at 10% of the production\\nlimit on usage for each workspace.\\nWhile this works with our usage pattern, setting good dev and staging limits may vary depending on\\nyour use case with LangSmith. For example, if you run evals as part of CI/CD in dev or staging, you may\\nwant to be more liberal with your usage limits to avoid test failures.\\nNow that our limits are set, we can see that LangSmith shows a maximum spend estimate across all workspaces:\\n\\nWith this estimator, we can be confident that we will not end up with an unexpected credit card bill at the end of the month.\\nSummary\\u200b\\nIn this tutorial, we learned how to:\\n\\nCut down our existing costs with data retention policies\\nPrevent future overspend with usage limits\\n\\nIf you have questions about further optimizing your spend, please reach out to support@langchain.dev.Was this page helpful?You can leave detailed feedback on GitHub.PreviousTutorialsNextAdministration how-to guidesProblem SetupUnderstand your current usageUsage GraphInvoicesOptimization 1: manage data retentionChange org level retention defaults for new projectsChange project level retention defaultsKeep around a percentage of traces for extended data retentionSee results after 7 daysOptimization 2: limit usageSetting a good total traces limitCutting maximum spend with an extended data retention limitSet dev/staging limits and view total spent limit across workspacesSummaryCommunityTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright © 2025 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45058ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Data--> Docs-->Divide our Docuemnts into chunks dcouments-->text-->vectors-->Vector Embeddings--->Vector Store DB\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents=text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98bcc869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='Optimize tracing spend on LangSmith | 🦜️🛠️ LangSmith\\n\\n\\n\\n\\n\\n\\nSkip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith. Join our team!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationTutorialsOptimize tracing spend on LangSmithHow-to GuidesSetupConceptual GuideSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceChangelogCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceAdministrationTutorialsOptimize tracing spend on LangSmithOn this pageOptimize tracing spend on LangSmith\\nRecommended ReadingBefore diving into this content, it might be helpful to read the following:\\nData Retention Conceptual Docs\\nUsage Limiting Conceptual Docs'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='noteSome of the features mentioned in this guide are not currently available in Enterprise plan due to its\\ncustom nature of billing. If you are on Enterprise plan and have questions about cost optimization,\\nplease reach out to your sales rep or support@langchain.dev.\\nThis tutorial walks through optimizing your spend on LangSmith. In it, we will learn how to optimize existing spend\\nand prevent future overspend on a realistic real-world example. We will use an existing LangSmith organization with high usage.\\nConcepts can be transferred to your own organization.\\nProblem Setup\\u200b\\nIn this tutorial, we take an existing organization that has three workspaces, one for each deployment stage\\n(Dev, Staging, and Prod):'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='Understand your current usage\\u200b\\nThe first step of any optimization process is to understand current usage. LangSmith gives two ways to do this: Usage Graph\\nand Invoices.\\nUsage Graph\\u200b\\nThe usage graph lets us examine how much of each usage based pricing metric we have consumed lately. It does not directly show\\nspend (which we will see later on our draft invoice).\\nWe can navigate to the Usage Graph under Settings -> Usage and Billing -> Usage Graph.\\n\\nWe see in the graph above that there are two usage metrics that LangSmith charges for:\\n\\nLangSmith Traces (Base Charge)\\nLangSmith Traces (Extended Data Retention Upgrades).'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content=\"The first metric tracks all traces that you send to LangSmith. The second tracks all traces that also have our Extended 400 Day Data Retention.\\nFor more details, see our data retention conceptual docs. Notice that these graphs look\\nidentical, which will come into play later in the tutorial.\\nLangSmith Traces usage is measured per workspace, because workspaces often represent development environments (as in our example),\\nor teams within an organization. As a LangSmith administrator, we want to understand spend granularly per each of these units. In\\nthis case where we just want to cut spend, we can focus on the environment responsible for the majority of costs first for the greatest savings.\\nnoteLangSmith's Usage Graph and Invoice use the term tenant_id to refer to a workspace ID. They are interchangeable.\\nIn the above image, the vast majority of usage is in the workspace with ID c27dd32c-7c80-4e8c-acde-bfcb67a90ab2. We can\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content=\"In the above image, the vast majority of usage is in the workspace with ID c27dd32c-7c80-4e8c-acde-bfcb67a90ab2. We can\\ngo to Settings -> Workspaces, and hover our mouse over the Workspace ID button to find the one with a matching ID. In\\nthis case, it's the Prod workspace:\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='Invoices\\u200b\\nWe understand what usage looks like in terms of traces, but we now need to translate that into spend. To do so,\\nwe head to the Invoices tab. The first invoice that will appear on screen is a draft of your current month\\'s\\ninvoice, which shows your running spend thus far this month.\\n\\nIn the above GIF, we see that the charges for LangSmith Traces are broken up by \"tenant_id\" (i.e. Workspace ID), meaning we can track tracing spend\\non each of our workspaces. In the first few days of June, the vast majority of the total spend of ~$2,000 is in our production\\nworkspace. Further, the majority of spend in that workspace was on extended data retention trace upgrades.\\nThese upgrades occur for two reasons:\\n\\nYou use extended data retention tracing, meaning that, by default, your traces are retained for 400 days\\nYou use base data retention tracing, and use a feature that automatically extends the data retention of a trace (see our Auto-Upgrade conceptual docs)'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content=\"Given that the number of total traces per day is equal to the number of extended retention traces per day, it's most likely the\\ncase that this org is using extended data retention tracing everywhere. As such, we start by optimizing our retention settings.\\nOptimization 1: manage data retention\\u200b\\nLangSmith charges differently based on a trace's data retention (see our data retention conceptual docs),\\nwhere short-lived traces are an order of magnitude less expensive than ones that last for a long time. In this optimization, we will\\nshow how to get optimal settings for data retention without sacrificing historical observability, and\\nshow the effect it has on our bill.\\nChange org level retention defaults for new projects\\u200b\\nWe navigate to the Usage configuration tab, and look at our organization level retention settings. Modifying this setting affects all new projects that are\\ncreated going forward in all workspaces in our org.\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='created going forward in all workspaces in our org.\\nnoteFor backwards compatibility, older organizations may have this defaulted to Extended. Organizations created after June 3rd\\nhave this defaulted to Base.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='Change project level retention defaults\\u200b\\nOur existing projects have not changed their data retention settings, so we can change these on the individual project pages.\\nWe navigate to Projects -> <your project name>, click the data retention drop down, and modify it to base retention. As\\nwith the organization level setting, this will only affect retention (and pricing) for traces going forward.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content=\"Keep around a percentage of traces for extended data retention\\u200b\\nWe may not want all our traces to expire after 14 days if we care about historical debugging. As such, we can take advantage\\nof LangSmith's built in ability to do server side sampling for extended data retention.\\nChoosing the right percentage of runs to sample depends on your use case. We will arbitrarily pick 10% of runs here, but will\\nleave it to the user to find the right value that balances collecting rare events and cost constraints.\\nLangSmith automatically upgrades the data retention for any trace that matches a run rule in our automations product (see our run rules docs). On the\\nprojects page, click Rules -> Add Rule, and configure the rule as follows:\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='Run rules match on runs rather than traces. Runs are single units of work within an LLM application\\'s API handling. Traces\\nare end to end API calls (learn more about tracing concepts in LangSmith). This means a trace can\\nbe thought of as a tree of runs making up an API call. When a run rule matches any run within a trace, the trace\\'s full run tree\\nupgrades to be retained for 400 days.\\nTherefore, to make sure we have the proper sampling rate on traces, we take advantage of the\\nfiltering functionality of run rules.\\nWe add add a filter condition to only match the \"root\" run in the run tree. This is distinct per trace, so our 10% sampling\\nwill upgrade 10% of traces, rather 10% of runs, which could correspond to more than 10% of traces. If desired, we can optionally add\\nany other filtering conditions required (e.g. specific tags/metadata attached to our traces) for more pointed data retention'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='any other filtering conditions required (e.g. specific tags/metadata attached to our traces) for more pointed data retention\\nextension. For the sake of this tutorial, we will stick with the simplest condition, and leave more advanced filtering as an\\nexercise to the user.\\nnoteIf you want to keep a subset of traces for longer than 400 days for data collection purposes, you can create another run\\nrule that sends some runs to a dataset of your choosing. A dataset allows you to store the trace inputs and outputs (e.g., as a key-value dataset),\\nand will persist indefinitely, even after the trace gets deleted.\\nSee results after 7 days\\u200b\\nWhile the total amount of traces per day stayed the same, the extended data retention traces was cut heavily.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content=\"This translates to the invoice, where we've only spent about $900 in the last 7 days, as opposed to $2,000 in the previous 4.\\nThat's a cost reduction of nearly 75% per day!\\n\\nOptimization 2: limit usage\\u200b\\nIn the previous section, we managed data retention settings to optimize existing spend. In this section, we will\\nuse usage limits to prevent future overspend.\\nLangSmith has two usage limits: total traces and extended retention traces. These correspond to the two metrics we've\\nbeen tracking on our usage graph. We can use these in tandem to have granular control over spend.\\nTo set limits, we navigate back to Settings -> Usage and Billing -> Usage configuration. There is a table at the\\nbottom of the page that lets you set usage limits per workspace. For each workspace, the two limits appear, along\\nwith a cost estimate:\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='Lets start by setting limits on our production usage, since that is where the majority of spend comes from.\\nSetting a good total traces limit\\u200b\\nPicking the right \"total traces\" limit depends on the expected load of traces that you will send to LangSmith. You should\\nclearly think about your assumptions before setting a limit.\\nFor example:\\n\\nCurrent Load: Our gen AI application is called between 1.2-1.5 times per second, and each API request has a trace associated with it,\\nmeaning we log around 100,000-130,000 traces per day\\nExpected Growth in Load: We expect to double in size in the near future.\\n\\nFrom these assumptions, we can do a quick back-of-the-envelope calculation to get a good limit of:\\nlimit = current_load_per_day * expected_growth * days/month      = 130,000 * 2 * 30      = 7,800,000 traces / month\\nWe click on the edit icon on the right side of the table for our Prod row, and can enter this limit as follows:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='noteWhen set without the extended data retention traces limit, the maximum cost estimator assumes that all traces are using extended data retention.\\nCutting maximum spend with an extended data retention limit\\u200b\\nIf we are not a big enterprise, we may shudder at the ~$40k per month bill.\\nWe saw from Optimization 1 that the easiest way to cut cost was through managing data retention.\\nThe same can be said for limits. If we only want to keep ~10% of traces to be around more than 14 days, we can set a limit on the maximum\\nhigh retention traces we can keep. That would be .10 * 7,800,000 = 780,000.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='As we can see, the maximum cost is cut from ~40k per month to ~7.5k per month, because we no longer allow as many expensive\\ndata retention upgrades. This lets us be confident that new users on the platform will not accidentally cause cost to balloon.\\nnoteThe extended data retention limit can cause features other than traces to stop working once reached. If you plan to\\nuse this feature, please read more about its functionality here.\\nSet dev/staging limits and view total spent limit across workspaces\\u200b\\nFollowing the same logic for our dev and staging environments, we set limits at 10% of the production\\nlimit on usage for each workspace.\\nWhile this works with our usage pattern, setting good dev and staging limits may vary depending on\\nyour use case with LangSmith. For example, if you run evals as part of CI/CD in dev or staging, you may\\nwant to be more liberal with your usage limits to avoid test failures.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='your use case with LangSmith. For example, if you run evals as part of CI/CD in dev or staging, you may\\nwant to be more liberal with your usage limits to avoid test failures.\\nNow that our limits are set, we can see that LangSmith shows a maximum spend estimate across all workspaces:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='With this estimator, we can be confident that we will not end up with an unexpected credit card bill at the end of the month.\\nSummary\\u200b\\nIn this tutorial, we learned how to:\\n\\nCut down our existing costs with data retention policies\\nPrevent future overspend with usage limits'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='Cut down our existing costs with data retention policies\\nPrevent future overspend with usage limits\\n\\nIf you have questions about further optimizing your spend, please reach out to support@langchain.dev.Was this page helpful?You can leave detailed feedback on GitHub.PreviousTutorialsNextAdministration how-to guidesProblem SetupUnderstand your current usageUsage GraphInvoicesOptimization 1: manage data retentionChange org level retention defaults for new projectsChange project level retention defaultsKeep around a percentage of traces for extended data retentionSee results after 7 daysOptimization 2: limit usageSetting a good total traces limitCutting maximum spend with an extended data retention limitSet dev/staging limits and view total spent limit across workspacesSummaryCommunityTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright © 2025 LangChain, Inc.')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65ba97d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings(model = 'mxbai-embed-large')\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstoredb=FAISS.from_documents(documents,embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1233687f",
   "metadata": {},
   "source": [
    "## Document Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b49b62e",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "\n",
    "- إنت هنا بتعمل Document Chain بحيث الـ LLM يجاوب عالسؤال بناءً على Context محدد (مش من دماغه)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90b6f279",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b3ba95",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "\n",
    "- جبت الأدوات اللي هتساعدك تدمج الدوكمنتات مع الموديل (document chain) وتبني prompt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47d2047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the following question based only on the provided context:\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c91dc5",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "\n",
    "- هنا بتقوله: اسمع مني، انت هتجاوب بس من الـ context اللي هدهولك.\n",
    "- الـ {context} ده مكان ما هيترمي النصوص الجاية. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c677b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Document Chain\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9121150",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "\n",
    "هنا عملت chain نوعه StuffDocumentChain.\n",
    "\n",
    "يعني: خد الـ context كله وارميه للـ LLM زي ما هو \"stuff\" (من غير تلخيص ولا تقسيم ذكي).\n",
    "\n",
    "بتربطه بالـ LLM بتاعك والـ Prompt اللي فوق."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04ea91d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The usage graph allows us to examine how much of each usage based pricing metric we have consumed lately.\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "document_chain.invoke({\n",
    "    \"input\": \"The usage graph lets us examine how much of each usage based pricing metric\",\n",
    "    \"context\": [\n",
    "        Document(page_content=\"The usage graph lets us examine how much of each usage based pricing metric we have consumed lately. It does not directly show spend (which we will see later on our draft invoice).\")\n",
    "    ]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af09dc6",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "\n",
    "هنا إنت بتديله:\n",
    "\n",
    "input: السؤال اللي اليوزر بيسأله.\n",
    "\n",
    "context: مجموعة الـ documents (ممكن تبقى أكتر من واحدة) اللي هيجاوب منها."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1891c8e",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "\n",
    "🔵 النتيجة:\n",
    "- \n",
    "الـ LLM هيجاوب على الـ input لكن مش من دماغه.\n",
    "\n",
    "هيجاوب بس من الـ context اللي انت بعته.\n",
    "\n",
    "وده المفيد في retrieval-based QA systems.\n",
    "- ال Document Chain بيقول للموديل: “ركز ف النص ده بس ومتخترعش من عندك”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9b41c7",
   "metadata": {},
   "source": [
    "#  Retrieval Chain & Document Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25c7a72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstoredb.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c916d50",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "\n",
    "بتحول الـ Vector Store (قاعدة بيانات متخزنة فيها Embeddings) إلى Retriever.\n",
    "\n",
    "الـ Retriever وظيفته:\n",
    "\n",
    "لما تسأله سؤال، يروح يجيبلك الـ Relevant Documents (اللي شبه السؤال)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d672431",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1322fd26",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "\n",
    "هنا بتعمل Retrieval Chain:\n",
    "\n",
    "أول حاجة: الـ retriever يجيبلك الدوكمنتات المناسبة.\n",
    "\n",
    "بعد كده: الـ document_chain (اللي شرحناه قبل كده) يبعت الـ context ده للـ LLM عشان يجاوب.\n",
    "\n",
    "يعني كده عملت End-to-End pipeline:\n",
    "\n",
    "سؤال → يروح يدور في الـ VectorDB → يجيب context → يبعت للـ LLM → يرد عليك."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "60761e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retrieval_chain.invoke({\n",
    "    \"input\": \"The usage graph lets us examine how much of each usage based pricing metric\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea816a5a",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "هنا فعليًا بتجرب الـ chain.\n",
    "\n",
    "بتمررله input (السؤال بتاعك).\n",
    "\n",
    "الـ chain بقى:\n",
    "\n",
    "هيستخدم الـ retriever يجيب سياق من الـ vectorstore.\n",
    "\n",
    "هيبعت السياق ده للـ document_chain.\n",
    "\n",
    "الـ LLM هيجاوب بناءً على السياق بس."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c190c525",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "\n",
    "🟢 الخلاصة:\n",
    "Retrieval Chain = Retriever + Document Chain\n",
    "\n",
    "✔️ Retriever: يدور عالسياق الصح.\n",
    "\n",
    "✔️ Document Chain: يرد على السؤال من السياق ده.\n",
    "\n",
    "✔️ كله أوتوماتيك.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "68c0f2bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here’s a summary of the information provided in the context:\\n\\n*   **LangSmith’s Usage Graph and Invoice** track usage metrics like traces and extended data retention traces.\\n*   **Trace Usage:** LangSmith charges for “Trace” metrics (all traces, including extended data retention) per workspace.\\n*   **Invoice Data Retention:** LangSmith charges $900 in the last 7 days, a cost reduction of 75% per day.\\n*   **Usage Limits:** LangSmith has two usage limits: total traces and extended data retention traces.\\n*   **Setting Limits:** You can set limits on each workspace by navigating to Settings -> Usage and Billing -> Usage Configuration.\\n*   **Data Retention Limits:**  Setting limits without extended retention limits assumes all traces are using extended data retention.\\n*   **Cost Optimization:**  The goal is to cut cost through limiting high retention traces to 10% of the total traces.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7fbdc173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'The usage graph lets us examine how much of each usage based pricing metric',\n",
       " 'context': [Document(id='8c83d891-d1d8-4b55-ab6a-55fead0ee243', metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='Understand your current usage\\u200b\\nThe first step of any optimization process is to understand current usage. LangSmith gives two ways to do this: Usage Graph\\nand Invoices.\\nUsage Graph\\u200b\\nThe usage graph lets us examine how much of each usage based pricing metric we have consumed lately. It does not directly show\\nspend (which we will see later on our draft invoice).\\nWe can navigate to the Usage Graph under Settings -> Usage and Billing -> Usage Graph.\\n\\nWe see in the graph above that there are two usage metrics that LangSmith charges for:\\n\\nLangSmith Traces (Base Charge)\\nLangSmith Traces (Extended Data Retention Upgrades).'),\n",
       "  Document(id='80a619f2-457f-4e17-8191-e680e6c17f6b', metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content=\"The first metric tracks all traces that you send to LangSmith. The second tracks all traces that also have our Extended 400 Day Data Retention.\\nFor more details, see our data retention conceptual docs. Notice that these graphs look\\nidentical, which will come into play later in the tutorial.\\nLangSmith Traces usage is measured per workspace, because workspaces often represent development environments (as in our example),\\nor teams within an organization. As a LangSmith administrator, we want to understand spend granularly per each of these units. In\\nthis case where we just want to cut spend, we can focus on the environment responsible for the majority of costs first for the greatest savings.\\nnoteLangSmith's Usage Graph and Invoice use the term tenant_id to refer to a workspace ID. They are interchangeable.\\nIn the above image, the vast majority of usage is in the workspace with ID c27dd32c-7c80-4e8c-acde-bfcb67a90ab2. We can\"),\n",
       "  Document(id='37109d2a-d0c1-468f-a565-dc0b97e96dd7', metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content=\"This translates to the invoice, where we've only spent about $900 in the last 7 days, as opposed to $2,000 in the previous 4.\\nThat's a cost reduction of nearly 75% per day!\\n\\nOptimization 2: limit usage\\u200b\\nIn the previous section, we managed data retention settings to optimize existing spend. In this section, we will\\nuse usage limits to prevent future overspend.\\nLangSmith has two usage limits: total traces and extended retention traces. These correspond to the two metrics we've\\nbeen tracking on our usage graph. We can use these in tandem to have granular control over spend.\\nTo set limits, we navigate back to Settings -> Usage and Billing -> Usage configuration. There is a table at the\\nbottom of the page that lets you set usage limits per workspace. For each workspace, the two limits appear, along\\nwith a cost estimate:\"),\n",
       "  Document(id='c358b2e5-e88c-4669-a611-d9eb234ce108', metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='noteWhen set without the extended data retention traces limit, the maximum cost estimator assumes that all traces are using extended data retention.\\nCutting maximum spend with an extended data retention limit\\u200b\\nIf we are not a big enterprise, we may shudder at the ~$40k per month bill.\\nWe saw from Optimization 1 that the easiest way to cut cost was through managing data retention.\\nThe same can be said for limits. If we only want to keep ~10% of traces to be around more than 14 days, we can set a limit on the maximum\\nhigh retention traces we can keep. That would be .10 * 7,800,000 = 780,000.')],\n",
       " 'answer': 'Here’s a summary of the information provided in the context:\\n\\n*   **LangSmith’s Usage Graph and Invoice** track usage metrics like traces and extended data retention traces.\\n*   **Trace Usage:** LangSmith charges for “Trace” metrics (all traces, including extended data retention) per workspace.\\n*   **Invoice Data Retention:** LangSmith charges $900 in the last 7 days, a cost reduction of 75% per day.\\n*   **Usage Limits:** LangSmith has two usage limits: total traces and extended data retention traces.\\n*   **Setting Limits:** You can set limits on each workspace by navigating to Settings -> Usage and Billing -> Usage Configuration.\\n*   **Data Retention Limits:**  Setting limits without extended retention limits assumes all traces are using extended data retention.\\n*   **Cost Optimization:**  The goal is to cut cost through limiting high retention traces to 10% of the total traces.'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "597b673a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='8c83d891-d1d8-4b55-ab6a-55fead0ee243', metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='Understand your current usage\\u200b\\nThe first step of any optimization process is to understand current usage. LangSmith gives two ways to do this: Usage Graph\\nand Invoices.\\nUsage Graph\\u200b\\nThe usage graph lets us examine how much of each usage based pricing metric we have consumed lately. It does not directly show\\nspend (which we will see later on our draft invoice).\\nWe can navigate to the Usage Graph under Settings -> Usage and Billing -> Usage Graph.\\n\\nWe see in the graph above that there are two usage metrics that LangSmith charges for:\\n\\nLangSmith Traces (Base Charge)\\nLangSmith Traces (Extended Data Retention Upgrades).'),\n",
       " Document(id='80a619f2-457f-4e17-8191-e680e6c17f6b', metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content=\"The first metric tracks all traces that you send to LangSmith. The second tracks all traces that also have our Extended 400 Day Data Retention.\\nFor more details, see our data retention conceptual docs. Notice that these graphs look\\nidentical, which will come into play later in the tutorial.\\nLangSmith Traces usage is measured per workspace, because workspaces often represent development environments (as in our example),\\nor teams within an organization. As a LangSmith administrator, we want to understand spend granularly per each of these units. In\\nthis case where we just want to cut spend, we can focus on the environment responsible for the majority of costs first for the greatest savings.\\nnoteLangSmith's Usage Graph and Invoice use the term tenant_id to refer to a workspace ID. They are interchangeable.\\nIn the above image, the vast majority of usage is in the workspace with ID c27dd32c-7c80-4e8c-acde-bfcb67a90ab2. We can\"),\n",
       " Document(id='37109d2a-d0c1-468f-a565-dc0b97e96dd7', metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content=\"This translates to the invoice, where we've only spent about $900 in the last 7 days, as opposed to $2,000 in the previous 4.\\nThat's a cost reduction of nearly 75% per day!\\n\\nOptimization 2: limit usage\\u200b\\nIn the previous section, we managed data retention settings to optimize existing spend. In this section, we will\\nuse usage limits to prevent future overspend.\\nLangSmith has two usage limits: total traces and extended retention traces. These correspond to the two metrics we've\\nbeen tracking on our usage graph. We can use these in tandem to have granular control over spend.\\nTo set limits, we navigate back to Settings -> Usage and Billing -> Usage configuration. There is a table at the\\nbottom of the page that lets you set usage limits per workspace. For each workspace, the two limits appear, along\\nwith a cost estimate:\"),\n",
       " Document(id='c358b2e5-e88c-4669-a611-d9eb234ce108', metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='noteWhen set without the extended data retention traces limit, the maximum cost estimator assumes that all traces are using extended data retention.\\nCutting maximum spend with an extended data retention limit\\u200b\\nIf we are not a big enterprise, we may shudder at the ~$40k per month bill.\\nWe saw from Optimization 1 that the easiest way to cut cost was through managing data retention.\\nThe same can be said for limits. If we only want to keep ~10% of traces to be around more than 14 days, we can set a limit on the maximum\\nhigh retention traces we can keep. That would be .10 * 7,800,000 = 780,000.')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe9d35c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
